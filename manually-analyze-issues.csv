project,issue_number,issue_type,text,classification,if_fixed,fixed_commit_hash
CAMEL,10048,summary,Memory leak in RoutingSlip,Code-LQualCd,yes,ddb852cdf7da29827fcab0b25a2b2ed6ee443cf9
CAMEL,10048,comment_1,"How to fix? I think, the best way is to remove this dangerous caching at all. There might be a temptation to implement equals() and hashCode() methods in the helper class in a way to delegate both these calls to the processor wrapped by this class. However, the root cause of the problem is the incorrect usage of a hash map. Key must implement equals() and hashCode(). We cannot require all implementations of Processor and RouteContext to implement these methods - it would be an unmotivated bloating of their contracts with irrelevant functionality. Error handlers in RoutingSlip are short-living objects, they shouldn't get into Old Gen, so GC will clean them without significant performance overhead.",Design-Des,yes,ddb852cdf7da29827fcab0b25a2b2ed6ee443cf9
CAMEL,10048,description,"RoutingSlip has a cache of error handlers implemented as a ConcurrentHashMap. This map stores error handlers as values, and uses some synthetic objects as keys. For some kind of destinations provided in routing slip, map lookup operation does not work. Hence, new error handlers are always added to the map and existing error handlers never reused. Finally, the program runs out of memory. The synthetic keys are actually instances of class Such key is based on two objects: RouteContext and destination Processor. Neither RouteContext nor Processor do not require their implementations to provide equals() and hashCode() methods. Strictly speaking, caching implementation in RoutingSlip is incorrect, because it uses hash map in the discouraged way. However, for some cases it works. The problem occurs when routing slip contains a 'sync' destination, in other words - destination is a Processor that does not implement AsyncProcessor interface. RoutingSlip determines destination producer via and the latter uses method. This method creates new instance of Processor for every processor that is not an instance of AsyncProcessor. This is where the problem hides: new object has different hash code (defined by Object.hashCode()) and new object isn't equal to the object used as a key in the hash map (well, Object.equals()). Finally, new key for the hash map is calculated, lookup operation cannot find this key in the hash map, new key-value pair is put into the hash map.",Design-Des,yes,ddb852cdf7da29827fcab0b25a2b2ed6ee443cf9
CAMEL,10476,comment_1,Let me know if I can provide any more information or if something isn't clear. I also was able to successfully test the workaround and I can run locally(in an IDE) using java:exec with the same properties file that the unit tests load by overriding and reading that config file. The only thing I am not confident in with the workaround is the difference between running in a DefaultCamelContext vs a in the IDE.,Design-Des,yes,35a8fb65ce2f4feffe0b663d4ed48fdf00e98f44
CAMEL,10476,description,"Problem: When running with a Camel Blueprint project a configAdminFile is not used to populate propertyplacehoders in when exectued with So a user can't run camel locally in a similar way to running in Karaf with file based property placeholder values. Workaround: I think, but haven't tested yet, that you can work around this locally using the methods described here: and/or how this solution appears to use exec:java locally and loads the properties via To reproduce the problem: Create a new project using (You need to change the log4j config to make it run.) To reduce the time, I created a project that runs here: Instead of using a default in the blueprint XML for the I setup the POM to include the following: In Camel 2.15.2 or earlier, this file would be loaded when mvn camel:run was invoked and the properties would be available via the PID at run time. After the changes made in CAMEL-9313, it appears that the method is only called in when the createTestBundle pathway is taken in java.lang.String, boolean, java.lang.String, java.lang.String, java.lang.String, So it appears test using get this functionality (as shown by the tests) but things executed from camel:run do not. Here you can see in Camel 2.14 that call to is made after the bundelContext is created. In the master branch version, that call is no longer made from main after the context is returned. I made a change locally to add a similar call to in Camel 2.18: Here is the output of the log statement from the example before this change: Here is the output of the log statement from the example after this change: As you can see before the change, the ${greeting} property is not poplulated via propertyplacehoder. After the change it is replaced. Given all the discussion of timing related issues in CAMEL-9313, I'm hesitant to say this is a good enough solution or that it aligns with the intention of the changes made in that fix. Given that configAdminFileName and configAdminPid are passed into perhaps the call to should happen inside createBundleContext or one of it sub-methods. Overall, I ""think"" a user should be able to use the configAdminPid and configAdminFileName settings to load properties via camel:run rather than work aound it, but I could be persumptious there.",Design-Des,yes,35a8fb65ce2f4feffe0b663d4ed48fdf00e98f44
CAMEL,10507,description,"Addressing the issue raised in the review, Jackson TypeReferences should be declared constant.",Code-LQualCd,yes,0c5786f4dc162fc6203145de2ae54da47fe780b0
CAMEL,10517,summary,Remove unnecessary SuppressWarnings,Code-LQualCd,yes,39a9f52232fcaa1d82b290622d24aa45523094d0
CAMEL,10678,description,We should use each individual fields instead of a string field eg use fields - from - to - name - model As individual fields. And use name as the index field if its intended to be unique. This allows users to better query/browse the registry from JMX as well to find which transformers there is. Also remember to adjust the code in the jolokia command.,Design-Des,yes,2bca939186660f1b0abdb98d7be17a6ec8e066e4
CAMEL,10950,description,"One of the drawbacks with the docker-java library used in the camel-docker component is that it depends on Jersey for the JAX-RS client. This can be problematic if you already have another JAX-RS implementation on the classpath. Therefore, it'd be nice if you could specify the that you want to work with. By default it'd be but you could choose to use the if you wanted to avoid Jersey. Similarly, users could implement their own and have camel-docker load this when it comes to build the Docker client.",Design-Des,yes,efdf259da9c2927251c7cf99d93e75a5d00c44ff
CAMEL,1107,description,"When using the options to configure httpClient using URI option, they should be removed from the uri that is left over to the HTTPProducer. should remove the httpClient.xxx so it's",Code-DedCd,yes,4bd210597d24567c52e2e397531c9037341becf3
CAMEL,1112,description,"when reading files from file,ftp/sftp component, it is often neccesarry to get the files in a predifined order (e.g. created date or filename). to solve this, an extra uri parameter could be used, to set the order of files beeing processed. ordering can be done after reading the file-list. i know this can also be solved by applying a resequencer pattern in the route, but i think for large files it would be hard to apply this. also a processing in an created date order would be more natural.",Design-Des,yes,4ef8b8962304f16061825f76a52c5cb6dcc6d29e
CAMEL,11171,description,"component has an issue with the usage of {{RAW()}} function in child endpoint configuration. will mishandle the the content of {{RAW()}} , when at some point the escaped ampersand symbol is unescaped, and a wrong set of parameters is used. The attached PR fixed the issues and adds a unit test to verify the behavior before and after the fix.",Test-LCvg,yes,d6088eb5175e4c66c7c8a467694a8d60ff8d3eb5
CAMEL,11196,comment_0,"We should also make it easy to reuse an existing connector and have separate configuration, eg so the same user can use a twitter-connector and then configure it for user A and user B in the same application without configuration clashes. Today the component level would reuse the same component / configuration. So we may want to find a way to clone a component or whatever we can think of. A little bit of problem is the generated spring boot auto configuration which is hard-coded to one prefix.",Design-Des,yes,3dd29006c3aa1c9612dc25461b64a4845e31138d
CAMEL,11196,description,"A Camel connector can be configured on two levels - component - endpoint Just like a regular Camel component. But we should allow users to configure a connector in one place, and not worry about if its component or endpoint level. And then let camel-connector when it prepares the connector figure out all of this for you. This may require supporting loading configuration from external resource files such as .properties files etc. Or something else. But from end users we can make this easier.",Design-Des,yes,3dd29006c3aa1c9612dc25461b64a4845e31138d
CAMEL,11282,description,We should extend the plain DefaultComponent (the is deprecated) ensure there is a plain default no-arg constructor so it makes using and configuring components easier. See eg SO If it was just a plain no-arg constructor then the bean style would have worked.,Design-Des,yes,420f06e6fc0c70d26b19e183e96d696c4114ffc6
CAMEL,11868,description,The current java transport client is due EOL in near future and it will be a good idea we switch the the new high level rest client instead. The new high level rest client is only released from version 5.6.0 and towards so an general upgrade of the dependencies is required for the ELK component. This also add the support for basic authentication without the need of x-pack. This is usefull when running ELK behind a reverse proxy.,Architecture-ObsTech,yes,928f185f9c5fcb1eb48d8f626b7f569210bca5f7
CAMEL,11868,comment_1,"It might be a good idea, since not all features is supported in the high level rest client yet I already got it to work with the rest client, but then I would suggest we create a super class for and a and In the we decide which one we should instantiate based on the type client on the class path. This of course mean the client dependency is not included in the the component. Regarding the it's not possible to use the Hight level REST client since it only support ELK from 5.6.0 and forward. One think to be aware of with the REST API that it is slightly different, which mean various request and response object is not called the same with the REST client. So either we have to document this in the manual or we have to come up with a common request / response object for both clients",Design-Des,yes,f929b940da164b4c4bd11638c8fe693c57f59c3a
CAMEL,11868,comment_2,"Can we not create a new component that use this new rest client only. And then the two older components can stay for older users, and eventually we can start marking them as deprecated because the java client is EOL from Elastic and the rest client is the future/way to go.",Design-Des,yes,f929b940da164b4c4bd11638c8fe693c57f59c3a
CAMEL,1196,summary,MockEndpoint - sleep for empty test doesnt work,Test-LacTst,yes,6bebc319bfb7375d78b50974814f1bb73f85756a
CAMEL,12104,comment_5,I think we can extend cxf Continuation interface a bit to add an isTimeout method so that we know the timeout happen and can handle this situation accordingly outside CXF,Design-Des,yes,5805a124ee5e76b61bf7bdb12ff0273669e2e730
CAMEL,12104,description,"There is very strange behavior in Camel cxf and cxfrs timeouts which could lead to sensitive data being released. Below is a code sample which illustrates the unexpected behavior. I think any developer would expect the test API to return ""Valid Response"" or some kind exception, but in fact it returns ""SENSITIVE DATA"" due to the default continuationTimeout of 30 seconds. This issue seems to have been introduced by",Design-Des,yes,c789e1f6b09b57d1b0882268ebbe8fecfa9c05b2
CAMEL,12104,summary,Unintuitive default cxf timeout behavior,Design-Des,yes,c789e1f6b09b57d1b0882268ebbe8fecfa9c05b2
CAMEL,12104,comment_0,Would you be able to build an unit test of this sample code so we can take that and add to the tests of camel-cxf and work on a fix.,Test-LacTst,yes,4a9a7c636732b4a92e54c9d6b181840723df56d4
CAMEL,1228,summary,Clean up the OSGI bundle profiles,Code-LQualCd,yes,235092f3a890d4bf573c6c9c71b69a2f67805b12
CAMEL,1256,comment_0,"A fix has been submitted. Here is some highlights. * The fix is for 2.0 only since it involves some APIs changes that is an overkill to keep it backward compatible. Will update wiki. * It depends on CXF 2.2-SNAPSHOT now. Will upgrade CXF once the next release of CXF is available. * CxfBinding, Bus [CAMEL-1239], can be looked up from registry by the ""#"" notation. * Decouple CXF Message from Camel Message. That is, users are no longer required to cast Camel's message body to CXF Message in order to access SOAP headers and body in PAYLOAD. In PAYLOAD mode, Camel message body now returns a new type CxfPayload which contains SOAP headers and body. With CxfPayload being the body in PAYLOAD mode, It makes using Camel converter to convert to other types pretty easy and transparent. * Cleaning up the old CxfBinding and ""invoking context"". Both tried to do Camel/CXF message binding. CxfBinding is now an interface that an custom impl can be set on each endpoint. impl can also be set on each endpoint. * [CAMEL-1254] support serviceClass=#bean * some major refactoring to make code cleaner.",Code-LQualCd,yes,9be406692d2b2a92d547dafbb93452c02a06bf4e
CAMEL,1256,description,"The camel-cxf component is dued for some code cleanup and refactoring. We can clean some of of the interfaces and redundant code, etc.",Code-DedCd,yes,9be406692d2b2a92d547dafbb93452c02a06bf4e
CAMEL,1256,summary,Clean up camel-cxf,Code-DedCd,yes,9be406692d2b2a92d547dafbb93452c02a06bf4e
CAMEL,12624,description,Currently we are running Camel AMQP component against Active MQ 5 (Amazon MQ) we want to move to an Artemis solution but this hasn't worked seamlessly. In CAMEL-9204 I believe a hardcoded topic prefix of topic:// was introduced I think for a workaround of an Active MQ 5 bug. In Artemis this means Camel connects to a topic named instead of and therefore receives no events. The only possible workaround is to manually create the connection factory which means then the topic prefix is not set. My believe is that the hardcoded topic prefix is a bug and should be removed or an option to override at the AMQP component level should be introduced. Bug location: AMQPComponent.java line 59 Workaround:,Design-Des,yes,d673acd3035365ef85aae9b6836ee489a8e6a18e
CAMEL,12646,description,"If you have complex types like and wants to allow to configure this via spring boot autoconfiguration in - then the generated spring boot classes with all the options will use getter/setter of types That seems correct, but the spring-boot tooling itself (that generates additional json file) will skip those as it only support primitives and string types. So we may need to fool, and generate the getter/setter as String type as you use it for configuring it as a bean reference by id anyway, eg = #myDataSource We can add in the javadoc that the type is",Design-Des,yes,11fe34439c6b16c652c7d5bb16e75bd998079280
CAMEL,12646,summary,camel-spring-boot - Auto configuration of complex types should be more tooling friendly,Design-Des,yes,11fe34439c6b16c652c7d5bb16e75bd998079280
CAMEL,1270,description,The countdown latch in MainSupport is not completed when Main is stopping. Then we have a hanging thread. Can bee seen using ctrl + \,Code-MTCor,yes,2ebe070905933331c175d51836b0334fe3a28013
CAMEL,13681,description,Any of the options you can configure via such as: camel.main.name And so on should be configurable via ENV variables which will override any existing configuration. This is good practice in containers and also how SB can do etc.,Design-Des,yes,68a93674a82e01b1d0c0312ff5998c05de94b8ae
CAMEL,1447,description,"See article, listening 2 We should consider renaming *handle* to *catchBlock* so its using the same name as try .. catch. And we could add a finallyBlock so end user got them all.",Code-LQualCd,yes,86e1a1c0ad9c97a3ab734739b6178eb4ea14c887
CAMEL,149,description,Brian McCallister had this great idea.. add a thread() method to the DSL instead of using the seda component to do async processing. We should be able to: or ThreadPoolExecutor pool = ...,Design-Des,yes,0ea40fae717241f5322d95b60233f982ae810040
CAMEL,1507,comment_0,might need some cleanup - but I have tested it and it works for me,Code-LQualCd,yes,6cb542d49d20c5fe62e5aed80787feef682821fa
CAMEL,1507,comment_4,I renamed the constant so its according to the Camel 2.0 syntax: rev 761182 on trunk,Code-LQualCd,yes,da37ef162241e8dd6ad9b08bb08b3e30f0cbc316
CAMEL,1507,description,"To send a email you have to follow a pretty specific course. This adds a property (which is poorly named in this patch) to the MailConfiguration that names the header that contains the plaintext version of the email, and adds a property where you can embed images inline. If an attachment has a filename starting with ""cid:"" then this will add the ""Content-ID"" header to that multipart body - which will allow the email client to put the image in the appropriate place when it is viewed. (i.e. the html email has something like <image src=""cid:0001"" /",Code-LQualCd,yes,0eec0b41e2005de2adf422785c3a5b86e5a8f4a1
CAMEL,1842,description,"camel-cxf and camel-mail are the popular components in camel, so we need add some OSGi integration test for them.",Test-LacTst,yes,7b7ab09e22500c6140fafbf59e46c5d3843571c7
CAMEL,1944,description,Currently only the camel-irc producer installs an event listener on the IRC connection to log incoming messages from the IRC server. This patch revamps the logging so that the component installs an event adapter to do logging before the connection is even established so it's easier to debug various issues that could be occurring with the connection to the IRC server.,Code-LQualCd,yes,7a3f1561f25770f35ead4c24d7896ab877398a01
CAMEL,2094,description,We now have option {{autoStartup}} on routes. Lets also have this on the camel context itself so it replaces the option that is a bit confusing. For example the code above will not start Camel on startup. You can then later start camel manually by And then all runs. This also means that we will remove the option so if you use this option you must migrate to the autoStartup instead.,Design-Des,yes,ea55e23638226420322851307a06cce597a65c44
CAMEL,251,description,"displays the file instead of directory (a boolean) automagically attempt to reconnect if from their destinations. This should probably be shared with Consumers, and could maybe done more cleanly using camel's try/catch features, but there you go. Smarter directory/file building/handling. This lets you handle URI's like similarly, avoid putting files in '/' on servers that expose the full filesystem, etc. More verbose logging. Stuff like ""what file went where,"" reconnection attempts, etc.",Code-LQualCd,yes,35c8dd74eb0f96c9d242234936fa4dc4d9252104
CAMEL,2535,description,"As we don't use the CxfSoap component any more, it's time to clean it up.",Architecture-ObsTech,yes,0d654ef76d63f65a81f465f2c5d02cc32e6a0b48
CAMEL,2535,summary,Get ride of the cxfsoap component,Code-DedCd,yes,0d654ef76d63f65a81f465f2c5d02cc32e6a0b48
CAMEL,2650,description,"This only applies for prototype scoped POJOs which uses @Produce or @Consume to inject a Camel Producer/Consumer. As the prototype scoped POJO is short lived and there could potentially be created many of those POJOs. Then @Produce / @Consume will inject new instances of Producer / Consumer as well. And since there is no standard way of knowing when the POJO is no longer in need, which is where we can to stop the Producer/Consumer. For singleton scoped this is not a problem as CamelContext will keep track on the created Producer/Consumer in its internal _servicesToClose_. Which is then stopped when CamelContext stops. For prototype we need a different strategy such as - proxy it to use pooled producers/consumers which CamelContext manage the lifecycle - use a shared ProducerTemplate / ConsumerTemplate instead which CamelContext manages the lifecycle - other - maybe some thread local tricks",Design-Des,yes,1ab7b8904e240361c4b8aa72427e9a606ecf9aa4
CAMEL,2682,description,This allows end users to continue routing the original message. Instead of now the just get the last splitted message which is confusing.,Design-Des,yes,4b00f97f5d1ae88f3224037675bfa91033e6c453
CAMEL,2879,description,"The file strategies was not designed to be as flexible as we may want them today, when introducing more and more options to dictate what should happens with the files when success or failure. As we got - noop - delete - move - moveFailed etc. you may combine then in ways in which we current doesn't support. You should be able to say that if OK then just delete the file, but if ERROR then move it to this error folder instead.",Design-Des,yes,5799e6d0b2bfcdb3fcafc87ce6a6849804fdefd1
CAMEL,2892,summary,remove ugly warnings running tests since upgrading to jetty 7,Code-DedCd,yes,6d8acc781a940470cf21cd196bdf995111f873d1
CAMEL,3048,description,The current sample is based on Direct component which is a bad sample as its a very special component. We should do a HelloWorldComponent - the producer will just print the message body to system out - the consumer is scheduled and triggers every 5th second with a dummy exchange,Design-Des,yes,df7ee396ac7c4f9beb917652a0c4e653a97e589b
CAMEL,3048,summary,Archetype for creating component should have better sample component,Design-Des,yes,df7ee396ac7c4f9beb917652a0c4e653a97e589b
CAMEL,3050,description,"Spring 3 changed internally how dependency resolution works. Its now worse as we have to play tricks in the Camel namespace handler to tell Camel the various pieces you can ref, should depend on Camel. Otherwise the dependency resolution in Spring is not working properly. This used to work like a charm in Spring 2.5.6.",Design-Des,yes,85340945239689f51f25d29bbb79727d728b2076
CAMEL,3077,description,"EhCache often has a bit of lag time when invalidating expired cache elements, first setting the Element value to null and then removing the key. If you are hitting a cache rapidly one often will run across a key that is present in the cache that still has a null element entry. The logic for successfully key retrieval just needs to be slightly tweaked to check for null values.",Design-Des,yes,1fab0408c5493502fcba171b11b66954d846e092
CAMEL,3077,summary,Cache Component needs to check for null values during GET operations,Design-Des,yes,1fab0408c5493502fcba171b11b66954d846e092
CAMEL,3100,summary,${file:length} should return 0 instead of null if the file length is 0,Code-LQualCd,yes,559993248cdaa590466e708652b1a5d4bcdf17b0
HADOOP,10106,comment_3,You seemed to have unnecessarily moved the doRead() function's location in the server.java file. Can you please leave it in its original place in the file and please resubmit the patch.,Architecture-VioMod,no,
HADOOP,10106,description,"INFO IPC Server listener on 8020: readAndProcess from client 10.115.201.46 threw exception Unknown out of band call #-2147483647 This is thrown by a reader thread, so the message should be like INFO Socket Reader #1 for port 8020: readAndProcess from client 10.115.201.46 threw exception Unknown out of band call #-2147483647 Another example is which can also be called by handler thread. When that happend, the thread name should be the handler thread, not the responder thread.",Code-MTCor,yes,763f073f41e3eaa9ecd11c6ec0b76234739272aa
HADOOP,10106,summary,Incorrect thread name in RPC log messages,Code-MTCor,yes,763f073f41e3eaa9ecd11c6ec0b76234739272aa
HADOOP,10169,comment_10,Thanks for updating the patch. One last nit is that this code: can be simplified to the following so there aren't so many return statements to track:,Code-LQualCd,yes,71b4903ea41a17c9305b25d24be786aed5b6e82f
HADOOP,10169,description,"When i looked into a HBase JvmMetric impl, just found this synchronized seems not essential.",Code-DedCd,yes,71b4903ea41a17c9305b25d24be786aed5b6e82f
HADOOP,10169,summary,remove the unnecessary synchronized in JvmMetrics class,Code-DedCd,yes,71b4903ea41a17c9305b25d24be786aed5b6e82f
HADOOP,10214,comment_0,"A simple fix. I think it was introduced by: r1556103 | vinodkv | 2014-01-07 09:56:11 +0800 (Tue, 07 Jan 2014) | 2 lines YARN-1029. Added embedded leader election in the ResourceManager. Contributed by Karthik Kambatla. The above change made public, so the multi-threaded warning be observed.",Code-MTCor,yes,519d5d3014c2a4c77d4b3b575bc34807e7c0ec50
HADOOP,10214,description,"When i worked at HADOOP-9420, found the unrelated findbugs warning:",Code-LQualCd,yes,519d5d3014c2a4c77d4b3b575bc34807e7c0ec50
HADOOP,10214,summary,Fix multithreaded correctness warnings in,Code-LQualCd,yes,519d5d3014c2a4c77d4b3b575bc34807e7c0ec50
HADOOP,10343,description,in we print logs at info level when we drop responses. This causes lot of noise on the console.,Code-LQualCd,yes,2417ca71d5115f16bd13a737087dab5edd04fb99
HADOOP,10353,description,"The class uses a plain {{HashMap}} for caching. When the number of inserted values exceeds the the map's load threshold, it triggers a rehash. During this time, a different thread that performs a get operation on a previously inserted key can obtain a null value instead of the actual value associated with that key. The result is a NPE potentially being thrown when calling protocol)}} concurrently.",Code-MTCor,yes,e5ccaa5d0341bc62d625a94a02b409efaca65051
HADOOP,10374,description,"There are valid use cases for accessing the InterfaceAudience annotations programatically. In HBase, we are writing a unit test to check whether every class in the client packages are annotated with one of the annotations. Plus, we are also thinking about a golden file to containing all public method sigs, so that we can ensure public facing API-compatibility from a unit test. Related: HBASE-8546, HBASE-10462, HBASE-8275",Test-LacTst,no,
HADOOP,10432,description,"The method is private and takes a configuration to fetch a hardcoded property. Having a public method to resolve a verifier based on the provided value will enable getting a verifier based on the verifier constant (DEFAULT, STRICT, STRICT_IE6, ALLOW_ALL).",Code-LQualCd,yes,9a2ec694fe6f1cf72b60d4f406b010bfa55ff04b
HADOOP,10485,comment_1,The v0 patch introduces no functionality changes. It only removes the unused classes.,Code-DedCd,yes,0862ee6520da2140e5f1b201042ae33adebc1943
HADOOP,10485,comment_2,"+1. Thank you, Haohui, for the code cleaning up.",Code-LQualCd,yes,0862ee6520da2140e5f1b201042ae33adebc1943
HADOOP,10485,description,Hadoop-streaming no longer requires many classes in o.a.h.record. This jira removes the dead code.,Code-DedCd,yes,0862ee6520da2140e5f1b201042ae33adebc1943
HADOOP,10485,summary,Remove dead classes in hadoop-streaming,Code-DedCd,yes,0862ee6520da2140e5f1b201042ae33adebc1943
HADOOP,10496,description,"{{FileSink}} opens a file. If the {{MetricsSystem}} is shutdown, then the sink is discarded and the file is never closed, causing a file descriptor leak.",Code-LQualCd,yes,9ac54b5480f7ac03d76f5a894eab87829123d2db
HADOOP,10496,summary,Metrics system FileSink can leak file descriptor.,Code-LQualCd,yes,9ac54b5480f7ac03d76f5a894eab87829123d2db
HADOOP,10499,comment_3,"Although it makes sense to remove an unused parameter, I wonder if the parameter should be honored? It would allow different rpc services to have different proxy superuser configurations as opposed to the current static configuration... It probably made sense when daemons generally had only one rpc service. I don't have a strong opinion because I suppose it could be by another jira.",Code-LQualCd,no,
HADOOP,10499,comment_6,"I believe , the static nature of the ProxyUser capability should be addressed separately, if there is a requirement for different services to have different ProxyUser capabilities. HADOOP-10448 would make it easier since it will move the state out of ProxyUsers. I am trying to clean up the interface exposed by _ProxyUsers_ in order to work on HADOOP-10448 and that's why this jira.",Code-LQualCd,no,
HADOOP,10499,comment_19,"Hi, Sorry for the inconvenience this caused. Is it possible to patch HBase to stop calling the 3-arg form of that was removed? The third argument was unused anyway, so this won't change any functionality in HBase. Additionally, this class is currently annotated {{Private}}, so I expected we didn't need to check for downstream impacts before I committed. That means one of two additional changes needs to happen: # Change HBase to stop calling altogether. I'm not really sure what alternative you would have though, so maybe this isn't feasible. # Change the annotation to {{LimitedPrivate}} for HBase, so devs know to check for downstream impacts on interface-breaking changes in the future. What are your thoughts on this?",Code-LQualCd,no,
HADOOP,10499,description,The Configuration parameter is not used in the authorize() function. It can be removed and callers can be updated. Attaching the simple patch which removes the unused _conf_ parameter and updates the callers. The ProxyUsers is defined as a private audience and so there shouldn't be any external callers.,Code-DedCd,yes,dd7d032457f93bf600a1322a34873b1142303da2
HADOOP,10499,summary,Remove unused parameter from,Code-DedCd,yes,dd7d032457f93bf600a1322a34873b1142303da2
HADOOP,10526,summary,Chance for Stream leakage in CompressorStream,Design-Des,yes,291af51b654e9de533e084042051ef6650b05fc2
HADOOP,10681,comment_1,"Added a SnappyCode impl into hive, which should come before in the classpath for hive+tez. Tested out TPC-H Query5, which has a spill-merge on the JOIN. Query times went from 539.8 seconds to 464.89 seconds, mostly from speedup to a single reducer stage.",Code-SlAlg,yes,8f9ab998e273259c1e7a3ed53ba37d767e02b6bb
HADOOP,10681,description,"The current implementation of SnappyCompressor spends more time within the java loop of copying from the user buffer into the direct buffer allocated to the compressor impl, than the time it takes to compress the buffers. The bottleneck was found to be java monitor code inside SnappyCompressor. The methods are neatly inlined by the JIT into the parent caller which unfortunately does not flatten out the synchronized blocks. The loop does a write of small byte[] buffers (each IFile key+value). I counted approximately 6 monitor enter/exit blocks per k-v pair written.",Code-SlAlg,yes,8f9ab998e273259c1e7a3ed53ba37d767e02b6bb
HADOOP,10681,comment_3,"The patch removes unsafe synchronized blocks from the individual methods in Snappy and Zlib codecs. This synchronization is slow and when used in the most common pattern for CompressionCodec is still thread-unsafe for sharing streams with loops running across multiple threads and would ideally require explicit code { while { 0, buffer.length); } } to get correct stateful behaviour. As code exists today it is not thread-safe and does slow lock-prefix x86_64 instructions. The JNI library below in SnappyCodec.c actually does its own locking mutexes for the actual critical sections within.",Design-Des,yes,8f9ab998e273259c1e7a3ed53ba37d767e02b6bb
HADOOP,10681,comment_7,Address findbugs warnings,Code-LQualCd,yes,8f9ab998e273259c1e7a3ed53ba37d767e02b6bb
HADOOP,10681,comment_17,Can we do the same for {{Lz4Compressor}} and I noticed a significant performance overhead using {{Lz4Compressor}} for compared to due to the same problem.,Code-SlAlg,no,
HADOOP,10729,description,"We have ProtocolInfo specified in protocol interface with version info, but we don't have unit test to verify if/how it works. We should have tests to track this annotation work as expectation.",Test-LacTst,yes,c4084d9bc3b5c20405d9da6623b330d5720b64a1
HADOOP,10748,description,Currently HttpServer2 loads the JspServlet by default. It should be removed as JSP support is no longer required.,Code-DedCd,no,
HADOOP,10930,summary,HarFsInputStream should implement PositionedReadable with thead-safe.,Code-MTCor,no,
HADOOP,10979,description,It would make adding common options to hadoop_usage output easier if some entries were auto-populated. This is similar to what happens in FsShell and other parts of the Java code.,Design-Des,yes,ee36f4f9b87b194c965a2c5ace0244ab11e1d2d6
HADOOP,11013,comment_1,a) adds --debug option b) reworks CLASSPATH export to be consistent/safer,Code-LQualCd,yes,d8774cc577198fdc3bc36c26526c95ea9a989800
HADOOP,11013,comment_6,"-01: * Replaced all of the if's with a function * Added more messages that fill in some of the blanks (e.g., when does get added?) * Because I'm never happy, even with my own code",Code-LQualCd,yes,d8774cc577198fdc3bc36c26526c95ea9a989800
HADOOP,11013,description,"As part of HADOOP-9902, java execution across many different shell bits were consolidated down to (effectively) two routines. Prior to calling those two routines, the CLASSPATH is exported. This export should really be getting handled in the exec function and not in the individual shell bits. Additionally, it would be good if there was: so that bash -x would show the content of the classpath or even a '--debug classpath' option that would echo the classpath to the screen prior to java exec to help with debugging.",Code-LQualCd,yes,d8774cc577198fdc3bc36c26526c95ea9a989800
HADOOP,11013,summary,"CLASSPATH handling should be consolidated, debuggable",Code-LQualCd,yes,d8774cc577198fdc3bc36c26526c95ea9a989800
HADOOP,11014,summary,Potential resource leak in due to unclosed stream,Code-LQualCd,yes,b351086ff66ca279c0550e078e3a9d110f3f36a5
HADOOP,11063,description,"Windows has a maximum path length of 260 characters. KMS includes several long class file names. During packaging and creation of the distro, these paths get even longer because of prepending the standard war directory structure and our share/hadoop/etc. structure. The end result is that the final paths are longer than 260 characters, making it impossible to deploy a distro on Windows.",Code-LQualCd,yes,b44b2ee4adb78723c221a7da8fd35ed011d0905c
HADOOP,11063,summary,"KMS cannot deploy on Windows, because class names are too long.",Code-LQualCd,yes,b44b2ee4adb78723c221a7da8fd35ed011d0905c
HADOOP,11103,summary,Clean up RemoteException,Code-LQualCd,yes,d4a2830b63f0819979b592f4ea6ea3abd5885b71
HADOOP,11117,comment_1,stack/message of little or no value,Code-LQualCd,yes,a469833639c7a5ef525a108a1ac70213881e627d
HADOOP,11117,comment_5,"I really wish we could catch these and not throw a stack trace to figure out why/what/where it broke. From an end user perspective, these stacks are completely unfriendly.",Code-LQualCd,yes,a469833639c7a5ef525a108a1ac70213881e627d
HADOOP,11117,comment_8,"Some of the test failures are spurious, the only regression appears to be These tests are failing because the test code is doing an {{assertEquals}} on the expected string; the propagation of the underlying exception message is breaking this comparison. Fix is what could have been done in the first place: use as the probe",Code-Oth,yes,a469833639c7a5ef525a108a1ac70213881e627d
HADOOP,11117,comment_9,patch -002 which patches the test to make it less brittle to exception strings,Code-LQualCd,yes,a469833639c7a5ef525a108a1ac70213881e627d
HADOOP,11117,description,"If something is failing with kerberos login, should fail with useful information. But not all exceptions from the inner code are caught and converted to LoginException. Those exceptions that aren't wrapped have their text and stack trace lost somewhere in the javax code, leaving on the text ""login failed"" and a stack trace of no value whatsoever.",Code-Oth,yes,a469833639c7a5ef525a108a1ac70213881e627d
HADOOP,11379,comment_0,In this patch I fixed warnings raised by findbugs 3 against hadoop-auth and They're all encoding related warnings.,Code-LQualCd,yes,6df457a3d7661a890e84fc89567f29d0fe23c970
HADOOP,11379,description,"When locally run findbugs 3.0, there are new warnings generated. This Jira aims to address the new warnings in hadoop-auth and",Code-LQualCd,yes,6df457a3d7661a890e84fc89567f29d0fe23c970
HADOOP,11379,summary,Fix new findbugs warnings in hadoop-auth*,Code-LQualCd,yes,6df457a3d7661a890e84fc89567f29d0fe23c970
HADOOP,11421,comment_0,"Thanks for working on this Colin. It'll be nice to swap this in where we can, JDK7 does a much better job at exposing filesystem APIs. I wonder if we should really return a ChunkedArrayList here. It only implements a subset of the AbstractList interface, and this is a pretty general-purpose method. For huge dirs, we should probably just be using the DirectoryStream iterator directly. I do see the use of these helper functions for quick-and-dirty listings though. I'd be okay providing variants of these functions that return a ChunkedArrayList, but it seems like the default should just be a normal ArrayList. Couple other things: * Need {{<p/* I read the docs at and it'd be nice to do like the example and unwrap the into an IOException.",Code-LQualCd,yes,9937eef7f7f04a7dd3d504ae7ec5852d488a1f6a
HADOOP,11421,comment_2,"I think maybe later will become more general-purpose. But you're right; for now, we better use {{ArrayList}}. ok Yeah, that's important... io errors should result in io exceptions. Looks like is a probably in order to conform to the {{Iterator}} interface. I removed the variant that returns a list of File, since I found that the JDK6 file listing interfaces actually returned an array of String, so returning a list of String is compatible-ish.",Code-LQualCd,yes,9937eef7f7f04a7dd3d504ae7ec5852d488a1f6a
HADOOP,11421,description,"We should have a drop-in replacement for File#listDir that doesn't hide IOExceptions, and which returns a ChunkedArrayList rather than a single large array.",Code-LQualCd,yes,9937eef7f7f04a7dd3d504ae7ec5852d488a1f6a
HADOOP,11523,comment_3,"Hi Duo. Thank you for the patch. The current patch would acquire the lease for all {{rename}} operations, covering both block blobs and page blobs. During initial development of atomic rename, we were careful to limit the scope to only page blobs (typically used by HBase logs). Existing applications using block blobs might not be expecting the leases, so I think we need to make sure the lease acquisition only happens after checking In the error handling, if an exception is thrown from the try block, and then another exception is also thrown from freeing the lease, then this would drop the original exception and throw the exception from freeing the lease. I suspect in general the main exception from the try block is going to be more interesting for root cause analysis, so I recommend throwing that one and just logging the one from freeing the lease, like so: In addition to the unit tests, I ran the test suite against a live Azure storage account and confirmed that everything passed. The release audit warning from Jenkins is unrelated to this patch.",Design-Des,yes,f2c91098c400da6db0f5e8e49e9bf0e6444af531
HADOOP,11523,comment_9,"Thanks, . I have just 2 more minor nitpicks. I suggest changing this: to this: This way, we'll log the full stack trace on lease free errors and have more information for troubleshooting. This is just a minor style point, but for the following, the keyword should be inline with the closing brace. Instead of this: We do this: Instead of this: We do this:",Design-Des,yes,f2c91098c400da6db0f5e8e49e9bf0e6444af531
HADOOP,11523,description,"In current WASB (Windows Azure Storage - Blob) implementation, when rename operation succeeds, WASB will update the parent folder's ""last modified time"" property. By default we do not acquire lease on this folder when updating its property and simply pass ""null"" to it. In HBase scenario, when doing distributed log splitting, there might be a case that multiple processes from different region servers will access the same folder, and randomly we will see this exception in regionserver's log, which makes log splitting fail. So we should acquire the lease when updating the folder property rather than pass ""null"" to it.",Design-Des,yes,f2c91098c400da6db0f5e8e49e9bf0e6444af531
HADOOP,11544,comment_2,Thanks for the comment . I removed the entry of from core-default.xml because the default (NeverSampler) is defined in I would like to left the description in Tracing.apt.vm because users still can use for setting trace sampler though the config key should be specified as + rather than in java code.,Code-LQualCd,yes,42548f4dc2b2e8ace891b970c2c6712c02bb43ea
HADOOP,11544,description,are no longer used.,Code-LQualCd,yes,42548f4dc2b2e8ace891b970c2c6712c02bb43ea
HADOOP,11544,summary,Remove unused configuration keys for tracing,Code-LQualCd,yes,42548f4dc2b2e8ace891b970c2c6712c02bb43ea
HADOOP,11607,comment_1,"+1 Note that as s3a uses SLF4J for its log API, it can switch to {{log.info(""item {}"", value)}} for terser/more efficient logging logic (we still recommend wrapping debug statements though, just to skip string creation. If you switch to that mode, I'd be even happier. But I'm OK with what you've done so far",Code-LQualCd,yes,aa1c437b6a806de612f030a68984c606c623f1d9
HADOOP,11607,description,"{{S3AFileSystem}} generates INFO level logs in {{open}} and {{rename}}, which are not necessary.",Code-LQualCd,yes,aa1c437b6a806de612f030a68984c606c623f1d9
HADOOP,11607,summary,Reduce log spew in S3AFileSystem,Code-LQualCd,yes,aa1c437b6a806de612f030a68984c606c623f1d9
HADOOP,11730,comment_4,"patch applied; tested against s3 EU. Given the nature of these problems, it may be good to start thinking about whether we can do things with better simulate failures; the test here is a good start, though we may want more complex policies...mockito might be the tool to reach for.",Test-LCvg,yes,19262d99ebbbd143a7ac9740d3a8e7c842b37591
HADOOP,11730,description,"s3n attempts to read again when it encounters IOException during read. But the current logic does not reopen the connection, thus, it ends up with no-op, and committing the wrong(truncated) output. Here's a stack trace as an example. It seems this is a regression, which was introduced by the following optimizations. Also, test cases should be reviewed so that it covers this scenario.",Test-LCvg,yes,19262d99ebbbd143a7ac9740d3a8e7c842b37591
HADOOP,11966,description,"HADOOP-11464 reinstated support for running the bash scripts through Cygwin. The logic involves setting a {{cygwin}} flag variable to indicate if the script is executing through Cygwin. The flag is set in all of the interactive scripts: {{hadoop}}, {{hdfs}}, {{yarn}} and {{mapred}}. The flag is not set through hadoop-daemon.sh though. This can cause an erroneous overwrite of {{HADOOP_HOME}} and inside hadoop-config.sh.",Code-LQualCd,no,
HADOOP,12135,comment_1,"-00: * removed hadoop hard-codes * added a flag to turn on/off asf license * supports multiple projects, but only as a single file * support for ranges of versions * more of my inability to write python",Code-LQualCd,yes,e8b62d11d460e9706e48df92a0b0a72f4a02d3f5
HADOOP,12135,comment_4,"(If someone wants to take this patch over and rewrite to be more pythonic, please do!)",Code-LQualCd,yes,e8b62d11d460e9706e48df92a0b0a72f4a02d3f5
HADOOP,12135,comment_5,+1 we can make things more pythonic in follow-ons.,Code-LQualCd,yes,e8b62d11d460e9706e48df92a0b0a72f4a02d3f5
HADOOP,12135,summary,cleanup releasedocmaker,Code-DedCd,yes,e8b62d11d460e9706e48df92a0b0a72f4a02d3f5
HADOOP,12368,description,"These are test base classes that need to be subclassed to run, can mark as abstract.",Code-LQualCd,yes,7ad3556ed38560585579172aa68356f37b2288c8
HADOOP,12520,description,"The hadoop-azure tests support execution against the live Azure Storage service if the developer specifies the key to an Azure Storage account. The configuration works by overwriting the file. This can be an error-prone process. The azure-test.xml file is checked into revision control to show an example. There is a risk that the tester could overwrite azure-test.xml containing the keys and then accidentally commit the keys to revision control. This would leak the keys to the world for potential use by an attacker. This issue proposes to use XInclude to isolate the keys into a separate file, ignored by git, which will never be committed to revision control. This is very similar to the setup already used by hadoop-aws for integration testing.",Design-Des,yes,73822de7c38e189f7654444ff48d15cbe0df7404
HADOOP,12701,comment_0,"After the fix, mvn for entire Hadoop tree took 01:27 min on my Macbook Pro. In comparison, it took 38.677 s before.",Code-SlAlg,no,
HADOOP,12701,description,Test source files are not checked by checkstyle because Maven checkstyle plugin parameter is *false* by default. Propose to enable checkstyle on test source files in order to improve the quality of unit tests.,Code-LQualCd,yes,e097c9a124dc0b9ae9076994d19663f29d771ef0
HADOOP,12733,comment_2,"RE: ASF license warnings This looks like a bunch of generated files, not checked in files.",Code-LQualCd,yes,01d31fe9389ccdc153d7f4bf6574bf8e509867c1
HADOOP,12733,description,The following variables appear to no longer be used.,Code-DedCd,yes,01d31fe9389ccdc153d7f4bf6574bf8e509867c1
HADOOP,12811,description,The HBase's HMaster port number conflicts with Hadoop kms port number. Both uses 16000. There might be use cases user need kms and HBase present on the same cluster. The HBase is able to encrypt its HFiles but user might need KMS to encrypt other HDFS directories. Users would have to manually override the default port of either application on their cluster. It would be nice to have different default ports so kms and HBase could naturally coexist.,Design-Des,yes,a74580a4d3039ff95e7744f1d7a386b2bc7a7484
HADOOP,12829,comment_0,"Attached a patch. Doesn't include any tests -- not sure exactly what to test. I internally tested by changing STATS_DATA_CLEANER to package-private and writing the following test: which passes with the change and hangs without it. I'm unclear on if hadoop even wants something like this, since I'm not up to speed on how hadoop handles JVM reuse for unit tests.",Test-LacTst,no,
HADOOP,12829,comment_1,"Thank you, . I can't think of any reason why this thread should swallow without a trace. It is not performing any operations that should inherently generate INE, as far as I can see, and if someone else sends an INE we ought to... interrupt the thread. Can you include the stack trace so that this exception has a better chance of getting noticed? Also, capitalize the error? +1 pending those changes",Code-LQualCd,yes,d9c409a4286e36387fb39e7d622e850c13315465
HADOOP,12829,description,"The implemented in HADOOP-12107 swallows interrupt exceptions. Over in Solr/Sentry land, we run thread leak checkers on our test code, which passed before this change and fails after it. Here's a sample report: And here's an indication that the interrupt is being ignored: This is inconsistent with how other long-running threads in hadoop, i.e. PeerCache respond to being interrupted. The argument for doing this in HADOOP-12107 is given as I'm unclear on what ""spurious wakeup"" means and it is not mentioned in So, I believe this thread should respect interruption.",Code-LQualCd,yes,d9c409a4286e36387fb39e7d622e850c13315465
HADOOP,12829,summary,swallows interrupt exceptions,Code-LQualCd,yes,d9c409a4286e36387fb39e7d622e850c13315465
HADOOP,12829,comment_11,"Hello guys, I hit the same thread leak issue in FLINK-15239. In our use case, a Flink cluster is started w/o Hadoop dependencies, and accepts jobs submitted from various clients. If a job needs to access Hadoop, client will include Hadoop jars as its dependencies. On Flink side, we create a class loader to load all the dependencies of each job. As a result, we start a thread each time we load the Hadoop classes. While the change here made the thread respond to interrupt exception, it seems there's no one to interrupt the thread. So I guess the thread only dies when the JVM exits. If that's the case, is it possible to provide a way to explicitly interrupt the thread?",Design-Des,no,
HADOOP,12888,comment_0,"Hadoop is notoriously bad for security manager support (See HADOOP-5731) ... though being able to go securely client-side would be good. As you note: server-side requirements shouldn't impact client side. # what happens if you try to use webhdfs rather than hdfs:// ? # show us the stack trace? Shell is used client-side to detect OS, there is a languishing patch to isolate OS checks...someone needs to refresh that patch and we can get it into trunk. It's also critical on windows. Which field is causing the problem?",Code-LQualCd,yes,5a725f0ab8ef9e2a8b08f088ba4e87531ae4530d
HADOOP,12888,comment_5,"attach the patch to this JIRA, use a name of the form hit the the ""submit patch"" button and it'll be trigger an automatic review. Yetus will complain about the lack of tests, but we'll have to go with that. Did it work for you in any manual tests Moving the issue from HDFS to Hadoop as its in the common module",Test-LacTst,no,
HADOOP,12888,comment_8,...afraid you'll have to look at the checkstyle issues. Don't worry about the test and whitespace complaints,Code-LQualCd,yes,5a725f0ab8ef9e2a8b08f088ba4e87531ae4530d
HADOOP,12888,comment_9,Fix checkstyle errors.,Code-LQualCd,yes,5a725f0ab8ef9e2a8b08f088ba4e87531ae4530d
HADOOP,12888,comment_11,Yet another update to fix checkstyle errors.,Code-LQualCd,yes,5a725f0ab8ef9e2a8b08f088ba4e87531ae4530d
HADOOP,12888,comment_15,"I went for debug: 1. to be consistent (similar to isSetsid 2. just like on windows on the other code paths, having these features disabled, on the client, has no impact. On info they would simply add noise and confusion in my opinion.",Code-LQualCd,yes,5a725f0ab8ef9e2a8b08f088ba4e87531ae4530d
HADOOP,12888,description,"HDFS _client_ requires dangerous permission, in particular _execute_ on _all files_ despite only trying to connect to an HDFS cluster. A full list (for both Hadoop 1 and 2) is available here along with the place in code where they occur. While it is understandable for some permissions to be used, requiring {{FilePermission <<ALL FILES To make matters worse, the code is executed to initialize a field so in case the permissions is not granted, the VM fails with which is unrecoverable. Ironically enough, on Windows this problem does not appear since the code simply bypasses it and initializes the field with a fall back value ({{false}}). A quick fix would be to simply take into account that the JVM {{SecurityManager}} might be active and the permission not granted or that the external process fails and use a fall back value. A proper and long-term fix would be to minimize the use of permissions for hdfs client since it is simply not required. A client should be as light as possible and not have the server requirements leaked onto.",Code-LQualCd,yes,5a725f0ab8ef9e2a8b08f088ba4e87531ae4530d
HADOOP,12923,description,Some code is used only by tests. Let's relocate them.,Architecture-VioMod,yes,1898810cda83e6d273a2963b56ed499c0fb91118
HADOOP,12923,summary,Move the test code in ipc.Client to test,Architecture-VioMod,yes,1898810cda83e6d273a2963b56ed499c0fb91118
HADOOP,12952,description,"The examples for building distributions include how to create one without any documentation. But it includes the javadoc stage in the build, which is very slow. Adding skips that phase, and helps round out the parameters to a build.",Code-SlAlg,yes,a107cee14bb5446057da81d1c95d7fffd759e497
HBASE,10001,description,"We have a mockup to test only the client. If we want to include the network, without beeing limited by the i/o, we don't have much tools. This coprocessor helps to test this. I put it in the main code as to make it usable without adding a jar... I don't think it's possible avoid the WAL writes in the coprocessors. It would be great to have it to simplify the test with any kind of client (i.e. w/o changing the durability).",Code-CpxCd,yes,d9b983bb5ee871387ce5f47d6890cf6ef4b9058d
HBASE,10008,description,"is flakey on Jenkins, this test has failed due to timeout a couple times for me on precommit. Here's the stacktrace.",Test-FlakyTst,yes,eab83c92ed9f101dc981933be8383590491dbff7
HBASE,10008,summary,is flakey on jenkins,Test-FlakyTst,yes,eab83c92ed9f101dc981933be8383590491dbff7
HBASE,1054,description,"I had some tables refuse to find their indexes even though they were defined and had been updated. Scanning .META. I see that some regions from the table don't have the indexes... Oddly, during the .META. scan, I would see that I had 3 entries per table. (I have very little data in each table, def not splitting yet) But when I visited the UI it showed just one region per table... This patch also addes some toStrings that helped in diagnostics, and a new null check that I found I needed in IndexedTableScanner",Design-Des,yes,7d7f84b6935f3980d3cc41df1a2be72abc8e9eb8
HBASE,10631,description,"There is an extra seek(0) on FileLink open, that we can skip",Code-LQualCd,yes,9f86836794cbb7addc3896082ff76ef02d3dd7bc
HBASE,10631,summary,Avoid extra seek on FileLink open,Code-LQualCd,yes,9f86836794cbb7addc3896082ff76ef02d3dd7bc
HBASE,10968,comment_1,Thanks Matteo. Patch also removes an unused local variable.,Code-DedCd,yes,89b60b0374102b9f7b21e04d0e248ff31ee11c6c
HBASE,10968,description,"Here is related code: context was dereferenced first, leaving the null check ineffective.",Design-Des,yes,89b60b0374102b9f7b21e04d0e248ff31ee11c6c
HBASE,10968,summary,Null check in is redundant,Design-Des,yes,89b60b0374102b9f7b21e04d0e248ff31ee11c6c
HBASE,11011,comment_3,"In this case is not bad, is an ""expected"" situation, where the RS died before moving the compacted file. Anyway, I think that all the loop can be removed, since is trying to lookup files in and if they are there are already loaded for sure.",Code-LQualCd,no,
HBASE,11011,comment_4,"This might be, but the log message doesn't convey any of that. Taken out of context (so most users reading our logs), it just says a file is missing. I'm trusting you on this one :)",Code-LQualCd,no,
HBASE,11011,comment_6,"none of the above, the removed code was doing nothing (see the comment inside the function). The output files in the entry are simply not useful",Code-LQualCd,no,
HBASE,11011,description,"On load we already have a StoreFileInfo and we create it from the path, this will result in an extra fs.getFileStatus() call. In we do a fs.exists() and later a fs.getFileStatus() to create the StoreFileInfo, we can avoid the exists.",Design-Des,yes,996ce5211cb228a508470f779e17f76168efe270
HBASE,11011,summary,Avoid extra getFileStatus() calls on Region startup,Design-Des,yes,996ce5211cb228a508470f779e17f76168efe270
HBASE,11229,description,"See parent issue. Small changes in the hit percentage can have large implications, even when movement is inside a single percent: i.e. going from 99.11 to 99.87 percent. As is, percents are ints. Make them doubles.",Design-Des,yes,46e53b089a81c2e1606b1616b1abf64277de50a9
HBASE,11229,summary,Change block cache percentage metrics to be doubles rather than ints,Design-Des,yes,46e53b089a81c2e1606b1616b1abf64277de50a9
HBASE,11511,comment_4,"Thanks Stack for taking a look. Good point. We writelock the updatesLock, then start the mvcc transaction, so it should be the case that sync() and sync(trx) should be the same thing I guess. But let me change it to be plain old sync() to be failure-proof. Indeed. The block above is a catch block for catching an ignoring any exception from writing the abort marker to WAL. The call to should still happen in the outer block, resulting in",Design-Des,yes,bbe29eb93cc819fbf0287aa2cb343649b72783bf
HBASE,11511,description,"We used to write COMPLETE_FLUSH event to WAL until it got removed in 0.96 in issue HBASE-7329. For secondary region replicas, it is important to share the data files with the primary region. So we should reintroduce the flush wal markers so that the secondary region replicas can pick up the newly flushed files from the WAL and start serving data from those. A design doc which explains the context a bit better can be found in HBASE-11183.",Design-Des,yes,bbe29eb93cc819fbf0287aa2cb343649b72783bf
HBASE,11621,comment_4,"Here was 0.98 test suite with patch - on the same host as the previous run: With patch, hbase-server module went from 66 min to 51 min.",Code-SlAlg,yes,62d51bb377af46c2fefd3e7a441579878744da13
HBASE,11621,description,Daryn proposed the following change in HDFS-6773: With this change in runtime for TestAdmin went from 8:35 min to 7 min,Code-SlAlg,yes,62d51bb377af46c2fefd3e7a441579878744da13
HBASE,11621,summary,Make MiniDFSCluster run faster,Code-SlAlg,yes,62d51bb377af46c2fefd3e7a441579878744da13
HBASE,12059,description,Different versions of hadoop have different annotations. We can smooth this out by providing our own.,Design-Des,yes,a5bd931682c10f34d5e33aa103eb64aeac632e03
HBASE,12106,description,Test annotation interfaces used to be under then moved to We should move them to,Code-LQualCd,yes,3557a3235210b178d9c6cd3355c68d0a7155243e
HBASE,12238,comment_0,In standalone mode this is a little disorientating... it shows up frequently:,Design-Des,yes,5062edebcfc18329360845988cfee5d25c23a934
HBASE,12238,description,"Let me fix a few innocuous exceptions that show on startup (saw testing 0.99.1), even when regular -- will throw people off. Here is one: More to follow...",Design-Des,yes,5062edebcfc18329360845988cfee5d25c23a934
HBASE,12238,summary,A few ugly exceptions on startup,Design-Des,yes,5062edebcfc18329360845988cfee5d25c23a934
HBASE,12464,comment_3,"It's not good for meta to stuck in FAILED_OPEN. Agree we should handle it differently. The patch looks good. Just couples things: 1. Can we add a log (info/debug level may be fine) when we reset the retry count to 0? 2. We also need to prevent meta region goes to FAILED_OPEN at method How about FAILED_CLOSE? It should be fine since the meta region is still available? Is this essentially the same as setting maximumAttempts to a huge number? In many cases, a region may not be able to heal automatically without a pill. Personally, I think a better monitoring system could be better in this case.",Design-Des,yes,7eefd0cbed094b188c0e68d3ee99d937fa6942b0
HBASE,12464,comment_4,"This looks good, except for Jimmy's comments above. Nit. should be renamed to Can you rebase the patch on top of current master code. FAILED_CLOSE should be fine I think. Not much to do there. We default to 10 attempts to assign. Maybe we should bump it up to 30 or so. I did not check how long we are sleeping overall, but for client -> server operations we are retrying 35 times for a total of 10 min before giving up. We can do the similar for region assignment.",Code-LQualCd,yes,7eefd0cbed094b188c0e68d3ee99d937fa6942b0
HBASE,12464,description,"meta table region assignment could reach to the 'FAILED_OPEN' state, which makes the region not available unless the target region server shutdown or manual resolution. This is undesirable state for meta tavle region. Here is the sequence how this could happen (the code is in Step 1: Master detects a region server (RS1) that hosts one meta table region is down, it changes the meta region state from 'online' to 'offline' Step 2: In a loop (with configuable maximumAttempts count, default is 10, and minimal is 1), AssignmentManager tries to find a RS to host the meta table region. If there is no RS available, it would loop forver by resetting the loop count (BUG#1 from this logic - a small bug) Step 3: Once a new RS is found (RS2), inside the same loop as Step 2, AssignmentManager tries to assign the meta region to RS2 (OFFLINE, RS1 = Based on the document ( ), this is by design - ""17. For regions in FAILED_OPEN or FAILED_CLOSE states , the master tries to close them again when they are reassigned by an operator via HBase Shell."". However, this is bad design, espcially for meta table region (it is arguable that the design is good for regular table - for this ticket, I am more focus on fixing the meta region availablity issue). I propose 2 possible fixes: Fix#1 (band-aid change): in Step 3, just like Step 2, if the region is a meta table region, reset the loop count so that it would not leave the loop with meta table region in FAILED_OPEN state. Fix#2 (more involved): if a region is in FAILED_OPEN state, we should provide a way to automatically trigger after a short period of time (leaving any region in FAILED_OPEN state or other states like 'FAILED_CLOSE' is undesirable, should have some way to retrying and auto-heal the region). I think at least for 1.0.0, Fix#1 is good enough. We can open a task-type of JIRA for Fix#2 in future release.",Design-Des,yes,7eefd0cbed094b188c0e68d3ee99d937fa6942b0
HBASE,12833,comment_11,"A couple of comments on this: 1) the interface changes and the managed/unmanaged connection issues are separate concerns. 2) On the mailing list you said: ""Yes, the concept of connection caching, aka, managed connections are going away."" If managed connections are going away, then let's make them go away across the board. If they're staying, then let's keep them and rework the interfaces to support managed connections as a first class citizen. I only see disadvantages for inconsistency in this aspect. That said, this can totally be something that I'm missing. Do you see any advantages to keeping a mix of managed and unmanaged connections around?",Design-Des,yes,af725a0357efa01f276f2fe15518e6e119b89e45
HBASE,12833,comment_13,"On closer inspection, it looks like SecurityAdmin and need updated with this new style as well. ReplicationAdmin appears to manage it's own connection by design, though the rational eludes me.",Code-LQualCd,yes,af725a0357efa01f276f2fe15518e6e119b89e45
HBASE,12833,comment_16,"I guess it could be, but won't connections still be leaked when these classes are used? All this shows is our test usage of this part of the client is not as through as others. Whatever you say .",Test-LCvg,yes,af725a0357efa01f276f2fe15518e6e119b89e45
HBASE,12833,description,TestShell is erring out (timeout) consistently for me. Culprit is OOM cannot create native thread. It looks to me like test_table.rb and hbase/table.rb are made for leaking connections. table calls for every table but provides no close() method to clean it up. test_table creates a new table with every test.,Code-LQualCd,yes,af725a0357efa01f276f2fe15518e6e119b89e45
HBASE,12833,summary,[shell] table.rb leaks connections,Code-LQualCd,yes,af725a0357efa01f276f2fe15518e6e119b89e45
HBASE,1298,description,"is a key in my ""userdata"" table which happens to be the start key for a region named lists a link to which is incorrect because the "" + "" character is not properly URI Encoded. This results in a misleading user error message: ""This region is no longer available. It may be due to a split, a merge or the name changed. "" manually escaping the "" + "" character as ""%2B"" produces the correct output. A quick skim of master.jsp suggests it has a similar problem: it doesn't URI Encode table names when constructing links to table.jsp",Code-LQualCd,yes,e2eacecb290eac0e7160d07231b9b05ae51ae96b
HBASE,13395,comment_5,It is deprecated in 1.0 I think. So we can remove it.,Code-DedCd,yes,50e9825139d9abfd280eb7a930c8c6a96e9e68a6
HBASE,13395,description,"This class is marked as deprecated, probably can remove it, and if any methods from this specific class are in active use, need to decide what to do on callers' side. Should be able to replace with just Table interface usage.",Code-DedCd,yes,50e9825139d9abfd280eb7a930c8c6a96e9e68a6
HBASE,1359,description,"If you see I removed and ip or something for security reasons Once I truncate the table, hbase freaks out for about 10 seconds and all the thrift servers die. Thrift server log: 2009-04-02 12:09:08,971 INFO Retrying connect to server: /:60020. Already tried 0 time(s). You see this a bunch of times and then it times out The hbase shell 13:01:08 INFO Quorum servers: Truncating t2; it may take a while Disabling table... 09/04/30 13:01:19 INFO client.HBaseAdmin: Disabled t2 0 row(s) in 10.3417 seconds Dropping table... 09/04/30 13:01:19 INFO client.HBaseAdmin: Deleted t2 0 row(s) in 0.1592 seconds Creating table... 0 row(s) in 14.7567 seconds undefined local variable or method `lsit' for #<Object:0x3bbe9a50 from (hbase):3 undefined local variable or method `lsit' for #<Object:0x3bbe9a50 from (hbase):4 null from `proc essRow' from `metaScan' from `metaScan' from `list Tables' from `listTables' from `invoke0' from `invoke' from `invoke' from `invoke' from ndling' from `invoke' from `call' from `cacheAndCal l' from `call' from `interpret' from `interpret' ... 113 levels... from `call' from `call ' from `call' from `cacheAndCal l' from `call' from `__file_ _' from `__file__ ' from `load' from `runScript' from `runNormally' from `runFromMain' from `run' from `run' from `main' from `list' from",Code-SlAlg,yes,87956cea5bd9866c9110a63dd0bbfb5e3e780262
HBASE,13710,description,HttpServer makes use of Hadoop's ReflectionUtil instead of our own. AFAICT it's using 1 extra method. Just copy that one over to our own ReflectionUtil.,Design-Des,yes,353b046d6c5cf028ed509d238eafa46b6b3ec7a3
HBASE,13905,comment_9,"Test is still in the flakey category, occasionally timing out with this stack",Test-FlakyTst,no,
HBASE,13905,description,The reruns failed because the test is not idempotent. Perhaps we should have the test startup clean up it's workspace before starting.,Design-Des,yes,35a9c509fb517ca846e2a376d7e7bdf30344d20f
HBASE,1396,summary,Remove unused sequencefile and mapfile config. from hbase-default.xml,Code-DedCd,yes,b8854372bc2400d7bde7cae4c2e97f9e2adee47d
HBASE,14494,comment_0,Simple patch which adds some commas to the help message for the follow shell commands: * * grant.rb * * revoke.rb,Code-LQualCd,yes,4da3c935d4a84968df62bb7e49833ad48a20ed87
HBASE,14494,description,"noticed that the grant shell command was missing commas in-between the method arguments in the usage/help messages. After taking a look at some of the others, it's not alone. I'll make a pass over the shell commands and try to find some more help messages which tell the user to run a bad command.",Code-LQualCd,yes,4da3c935d4a84968df62bb7e49833ad48a20ed87
HBASE,14494,summary,Wrong usage messages on shell commands,Code-LQualCd,yes,4da3c935d4a84968df62bb7e49833ad48a20ed87
HBASE,14517,comment_5,Fix checkstyle errors,Code-LQualCd,yes,445dbd8a0e45e45bd4a44dee999a6c134c374dfc
HBASE,14517,description,"In production env, regionservers may be removed from the cluster for hardware problems and rejoined the cluster after the repair. There is a potential risk that the version of rejoined regionserver may diff from others because the cluster has been upgraded through many versions. To solve this, we can show the all regionservers' version in the server list of master's status page, and highlight the regionserver when its version is different from the master's version, similar to HDFS-3245 Suggestions are welcome~",Design-Des,yes,445dbd8a0e45e45bd4a44dee999a6c134c374dfc
HBASE,14604,comment_1,Any chance of a unit test ?,Test-LacTst,yes,60465964039acd05f43f268cdb4f909a150a0f41
HBASE,14604,comment_4,The classes are in the same package. Please make the new methods package private. Patch for 0.98 branch should contain 0.98 in the filename. Extension should be either .txt or .patch. Please attach patch for master branch.,Code-LQualCd,yes,60465964039acd05f43f268cdb4f909a150a0f41
HBASE,14604,description,"The code in MoveCoseFunction: It uses cluster.numRegions + META_MOVE_COST_MULT as the max value when scale moveCost to [0,1]. But this should use maxMoves as the max value when cluster have a lot of regions. Assume a cluster have 10000 regions, maxMoves is 2500, it only scale moveCost to [0, 0.25]. Improve moveCost by use maxMoves) as the max cost, so it can scale moveCost to [0,1].",Design-Des,yes,60465964039acd05f43f268cdb4f909a150a0f41
HBASE,15192,comment_6,Looks like there are other flaky sub-tests.,Test-FlakyTst,yes,243e6cc5293dc1e2a4dfd3af4ee29087c84184c8
HBASE,15192,comment_9,"If there is no objection, I plan to resolve this JIRA. There is still flaky subtest, e.g. : which we can track in separate issue(s)",Test-FlakyTst,yes,243e6cc5293dc1e2a4dfd3af4ee29087c84184c8
HBASE,15192,description,"fails intermittently due to failed assertion on cleaned merge region count: Before calling the test does: newcount1 is not cleared at the beginning of the loop. This means that if the check for newcount1 <= 1 doesn't pass the first iteration, it wouldn't pass in subsequent iterations. After timeout is exhausted, is called. However, there is a chance that has been called by the Chore already (during the wait period), leaving the cleaned count 0 and failing the test.",Test-FlakyTst,yes,243e6cc5293dc1e2a4dfd3af4ee29087c84184c8
HBASE,15192,summary,is flaky,Test-FlakyTst,yes,243e6cc5293dc1e2a4dfd3af4ee29087c84184c8
HBASE,15617,comment_1,"The new depends on some internal details likely to change. We should treat ZK state as a black box and avoid relying on it. This is a layering violation, really. Why not use the supported Admin API's to get the live server list to enumerate? That unit test failure appears unrelated.",Design-Des,yes,460b41c80012eff4ba59be95a86037d3b9c736b4
HBASE,15617,description,"When running in regionserver mode the Canary is expected to probe for service health one time per regionserver during a probe interval. Each time the canary chore fires, we create a which uses (via to enumerate over all table descriptors, find the locations for each table, then assemble the list of regionservers to probe from this result. The list may not contain all live regionservers, if there is a regionserver up but for some reason not serving any regions. To ensure we have the complete list of live regionservers I think it would be better to use and enumerate the live server list returned in the result.",Design-Des,yes,460b41c80012eff4ba59be95a86037d3b9c736b4
HBASE,15640,comment_0,"Here is small patch which ups our limit to 1M blocks from 100k so less likely we'll hit limit and then if we do, puts in place a message in red with how to address the limit (I couldn't figure what the previous message was trying to say... didn't seem to make sense). This could be better.. but will do for now.",Design-Des,yes,6dd938c20bf4634b5efa1ceca4ded7e54f8d9c0e
HBASE,15640,description,"I was baffled looking at L1 Cache because it was stuck at 100k blocks and only 'loading' 3G of data according to the UI. When I looked in log, all the numbers looked write in the Cache summary reported in log. Turns out our buckecache UI template will cut off calculating stats at a limit --which is probably what we want -- but the fact that it has done this should be more plain.",Design-Des,yes,6dd938c20bf4634b5efa1ceca4ded7e54f8d9c0e
HBASE,15640,summary,L1 cache doesn't give fair warning that it is showing partial stats only when it hits limit,Design-Des,yes,6dd938c20bf4634b5efa1ceca4ded7e54f8d9c0e
HBASE,15835,comment_5,"Curious: It appears that TestFSHLogProvider is (inadvertently, I imagine) instantiating an HBaseTestingUtility with a *null* Configuration (this within the context of kicking off a thread). This might be worthy of further investigation (since it's probably not intended to be this way), but for now I will simply precede my newly inserted logic with a check for a null Configuration, and thus avoid the that my original patch encountered.",Design-Des,yes,e6d613de70decf2b0f21d61bfa066cca870d5a09
HBASE,15835,comment_11,"Okay, the above-listed QA-failure prompted me to read in more detail those emails regarding the problem of ""flaky|flakey"" tests. I see that is indeed on the list, and also that testing of HBASE-15876 ran into exactly the same (apparently ""flaky|flakey"") failure. In this case, an awkward aspect is that the changes I made could conceivably have caused this kind of a failure!! So, just for good measure, I did a fresh clone/install of the Master branch and ran the same test: yep, it failed in exactly the same way.",Test-FlakyTst,yes,e6d613de70decf2b0f21d61bfa066cca870d5a09
HBASE,15835,description,"When a MiniCluster is being started with the method (most typically in the context of JUnit testing), if a local HBase instance is already running (or for that matter, another thread with another MiniCluster is already running), the startup will fail with a RuntimeException saying ""HMasterAddress already in use"", referring explicitly to contention for the same default master info port (16010). This problem most recently came up in conjunction with HBASE-14876 and its sub-JIRAs (development of new HBase-oriented Maven archetypes), but this is apparently a known issue to veteran developers, who tend to set up the @BeforeClass sections of their test modules with code similar to the following: A comprehensive solution modeled on this should be put directly into HBaseTestUtility's main constructor, using one of the following options: OPTION 1 (always force random port assignment): OPTION 2 (always force random port assignment if user has not explicitly defined alternate port):",Design-Des,yes,e6d613de70decf2b0f21d61bfa066cca870d5a09
HBASE,1655,comment_1,"Patch looks pretty good. Small nitpicky issues with some needless reorderings of stuff and also some tabbing oddities. Can be fixed on commit. I remember why we're using TreeMap now instead of HashMap. HashMap with byte[] as a key does not work with default comparator, and you cannot pass one in. There is no real downside to using TreeMap unless you have hundreds or thousands of tables, and even then logarithmic on the client-side on the order of 100-1000 is negligible. More efficient in memory but a little dirtier on the GC... however this is almost exclusively client-side (and there's ever only one) so really makes no difference IMO. So, back to TreeMap. The API seems to have grown quite a bit. Do we need all these permutations of getTable() ? Could we drop the String taking ones besides the simplest one? (This is why we don't support String in most of the HBase API now, leads to very long and ugly APIs) The default getTable(byte [] tableName) also requires instantiating a new each time internally, even if we are reusing an existing HTable... Whether there is a significant overhead or not to that, we should avoid it when unnecessary. Typical usage is probably to not supply your own HBC, so if someone wants that level of control, give them the ability to build the Pool themselves rather than expose the friendlier, higher-level methods with all the different ways to instantiate the pool. Blah, this is better said in a new patch....",Code-LQualCd,yes,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
HBASE,1655,comment_2,"This doesn't include any stargate stuff, only issues there are the tab issues, reordering, and whether to expose the pools or not. I'm not sure we want to remove the ability to work with HTablePool directly or not. This patch re-publics a bunch of stuff, cuts down on the API quite a bit, and reduces re-instantiation of HBC when getting pools that already exist and will always reuse the original when getting tables. We should definitely keep that. Otherwise the changes are open to discussion, I'm not particularly sold on my own patch, just providing it so we have a point of reference for discussion. The biggest thing to figure out is whether we even expose HTablePool non-statically to the user. If not, we go down the path of a much longer easy-to-use API... /me sleeping on it",Code-LQualCd,yes,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
HBASE,1655,comment_4,"A few questions/comments on the comments: - Why does the key to a HashMap need a comparator? - I much prefer the type of String vs byte[] for the tableName throughout all the method signatures and as a key to the internal map, but I tried to keep it as-is because it seemed to fit more with other HBase code. Can we just change it to String throughout and go back to the HashMap? - I am not a fan of the current API either. I'd much prefer that the HTablePool be instantiated by a client and we get rid of the static methods and static Map altogether. This fits in much more nicely for people like me who are using IoC containers like Spring. It also allows the ability to have multiple HTablePool instances, maybe each with their own configuration (which is currently just the max size). - Sorry about the tab/spaces issue. I didn't clean it up carefully enough. - Sorry about the reordering of imports. I use Eclipse which does this automatically and I was too lazy to try to restore the order back to the way it was. I'll try harder next time. So what is the next step? Should I make a new patch with an HTablePool that is meant to be instantiated and w/o the static methods?",Code-LQualCd,yes,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
HBASE,1655,comment_6,"- Even though TreeMap uses the comparator rather than the equals method to compare keys, using a byte[] as the key seems to break the contract of a java.util.Map. The last paragraph of the [Map says that impls are free to avoid calling equals() but the containsKey(Object key) method should ""return true if and only if this map contains a mapping for a key k such that (key==null ? k==null : key.equals(k))"". As you mentioned, you'd need a byte[] wrapper in order to be compliant. So a byte[] key will work in a TreeMap (and not in a HashMap), but we'd break the Map contract. Seems better to just use String and HashMap which works well and satisfies the Map contract. - Allowing both the static methods and the public construction would bring me back to the complaint I had originally with this class. :) It becomes confusing for a user trying to quickly understand how he/she is supposed to interact with this class. I guess this can all be explained away with documentation, but, it doesn't feel ""good"". - Also, yes, there would be a problem with re-instantiating the static map multiple times, but this could be prevented by implementing a Singleton pattern so that the static access operates on an internal singleton instance with a member Map rather than a static Map. - I agree that there is no reason to construct HBC's for each invocation. An internal HBC instance can be constructed and reused. - Is there a checkstyle.xml file for HBase? That would make it easy to check my code for formatting problems before submitting patches.",Design-Des,yes,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
HBASE,1655,comment_7,".bq So a byte[] key will work in a TreeMap (and not in a HashMap), but we'd break the Map contract. Seems better to just use String and HashMap which works well and satisfies the Map contract. Looking at the TreeMap equals implementation, yes, it breaks the above contract for Map. We're impure (insert lots of sack cloth and ashes, self-flagellation, etc., here -- smile). We are zealous about using byte [] everywhere. As Jon allows above, client-side, we can relax some. IMO, we should not be allowing public construction and static methods. +1 on Singleton to prevent multiple instantiations. There is no checkstyle (maybe there is one up in hadoop). In general 80 characters per line and two spaces for tab. Anything else we'll take. Don't worry about it too much.",Design-Des,yes,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
HBASE,1655,comment_11,"I've submitted another patch, that reflects a different approach taken to improve HTablePool. In the new approach, there will be no static methods at all. Instead, the normal usage of the class will be to instantiate an HTablePool directly and use the getTable and putTable methods to get an HTable and return an HTable to the pool. The pool class will automatically instantiate a new HTable in the case that no HTable for the given table name is in the pool OR if the internal pool for that table name is full (has more elements than its configured max size). The default max size is Integer.MAX_VALUE. While we lose the convenience of calling static methods, we gain a design with less code that is easy to understand, easy to use in an IoC container, and easy to unit test. This patch also includes a small modification to HTable that allows an instantiation to HTable with a null HBaseConfiguration object. This makes it possible to unit test classes such as HTablePool that internally use an HTable without requiring an environment in which the HTable can get and use a connection. Finally, this patch includes modifications to the stargate package reflecting the new HTablePool API. Let me know if this is suitable for inclusion in the project. Thanks to Jon Gray and Stack for their help and input.",Design-Des,yes,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
HBASE,1655,description,"A discussion on the HBase user mailing list led to some suggested improvements for the class. I will be submitting a patch that contains the following changes to HTablePool: * Remove constructors that were not used. * Change access to remaining contstructor from public to private to enforce use of the static factory method getPool. * Change internal map from TreeMap to HashMap because I couldn't see any reason it needed to be sorted. * Remove HBaseConfiguration and tableName member variables since they aren't really properties of the pool itself. They are associated with the HTable that should get instantiated when one is requested from the pool, but not already there.",Code-DedCd,yes,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
HBASE,1655,summary,Usability improvements to HTablePool,Design-Des,yes,6f242ea63f200fe07dd23fc35984e7a9fd7d8215
HBASE,16856,summary,Exception message in SyncRunner.run() should print currentSequence,Design-Des,yes,278625312047a2100f4dbb2d2eaa4e2219d00e14
HBASE,17101,comment_0,"including a rough patch, will cleanup and upload.",Code-LQualCd,yes,680289d67deb42922dd244daa11496a4c8a38f80
HBASE,17101,description,"As described in the doc (see HBASE-15531), we would like to start with user tables for favored nodes. This task ensures FN does not apply to system tables. System tables are in memory and won't benefit from favored nodes. Since we also maintain FN information for user regions in meta, it helps to keep implementation simpler by ignoring system tables for the first iterations.",Design-Des,yes,680289d67deb42922dd244daa11496a4c8a38f80
HBASE,17184,description,Initially I only wanted to fix a badly worded Log message but found a bunch of code convention violations etc. so I decided to clean up the class. It is only whitespace changes with the exception of the one log message (too many).,Code-LQualCd,yes,4a20361f1a235e0365c7923e64d20b28166cfd4b
HBASE,17184,summary,Code cleanup of LruBlockCache,Code-LQualCd,yes,4a20361f1a235e0365c7923e64d20b28166cfd4b
HBASE,17338,comment_3,"In case of on heap MSLAB, all the size calc doing global memstore data size + heap overhead based. That is why was not adding the cell data size in case of on heap. But I think I can do it bit diff way.. So instead of tracking the data size and heap overhead, we will track cell data size and heap size. Pls see it is not heap overhead alone.. This is the total heap space occupied by memstore(s). I can change the calc accordingly. For on heap MSLAB , this heap space count will be the same as what we have before off heap write path work (ie. It will include all cell data size and overhead part). The new accounting ie. cellDataSize will include only the cell data bytes size part. This will be a subset of the former one then. Will do all necessary changes.. This will be a bigger patch then as I will rename all the related area from heapOverhead to heapSize or so. I believe that way will look cleaner. Thoughts? Append/Increment is not adding cells into MSLAB area.. This is to avoid MSLAB wastage. Say same cell is getting incremented 1000 times and cell key+ value size is 100 bytes. If every increment (add to memstore) cell was added to MSLAB, we will overall take ~100KB MSLAB space whereas only 100 bytes is valid at any point.. All the former cells are getting deleted by the addition of a new cell.. The Cell POJO as such is removed from CSLM. But we can not free up that space in MSLAB.. MSLAB is not designed to do this way. That the chunk allocation and offset allocation within a chunk is serially incremented way happening. We can not mark some in btw space as free and reuse.. That will make things very complex for us.. So to avoid these, the simplest way was to NOT use MSLAB for upsert.",Design-Des,yes,ab5970773ad187871270bb63f9a37cead4cf2f6a
HBASE,17338,comment_8,+1 Add more comment on commit to MemstoreSize about diff between heap and data size.... (can be a version of comment that is later in class...) Glad of the simplification. Good stuff @anoop sam john,Code-CpxCd,yes,ab5970773ad187871270bb63f9a37cead4cf2f6a
HBASE,17338,comment_9,Rest is all good. But as discussed internally that isOffheap() addition to MSLAB is better so that we can really avoid adding the dataSize to the MemstoreSize when the memstore is offheap but still the entire Cell is onheap. Now currently we just account for the dataSize also. We can do in another JIRA. +1 otherwise.,Design-Des,yes,ab5970773ad187871270bb63f9a37cead4cf2f6a
HBASE,17338,comment_10,"Thanks for the reviews Stack & Ram.. I have to fix the failed test as those were having assert on heapOverhead size after cell addition to Memstore. Now we track heapSize not overhead. Will add more comments as Stack asked Regarding Ram's comment. I have different opinion. I dont think we need change this way. We track dataSize (irrespective of cell data in on heap or off heap area).. This dataSize been used at Segment level for in memory flush decision, Region level for on disk flushes and globally to force flush some regions. At the 1st 2 levels, it is not doubt that we have to track all the cell data size together. Now the point Ram says is when we have off heap configured and max off heap global size is say 12 GB, once the data size globally reaches this level, we will force flush some regions. So his point is for this tracking, we have to consider only off heap Cells and on heap Cell's data size should not get accounted in the data size but only in the heapSize. (At global level. But at region and segment level it has to get applied). 2 reasons why I am not in favor of this 1. This makes the impl so complex. We need to add isOffheap check down the layers. Also at 2 layers we have to consider these on heap cell data size and one level not. 2. When off heap is enabled, (We have the MSLAB pool off heap in place), we will end up in having on heap Cells when the pool is out of BBs. We will create on demand LABs on heap. If we dont consider those cell's data size at global level, we may reach forced flush level a bit late. That is the only gain. But here the on demand LAB creation is a sign that the write load is so high and delaying the flush will add more pressure to the MSLAB and more and more on demand BBs (2 MB sized) need to get created. One aim of the off heap work is to reduce the max heap space need for the servers. So lets consider the cell data size globally also (how we do now) and make global flushes. Now even if MSLAB is used, the append/increment use cases wont use MSLAB. The cells will be on heap then. But for such a use case, enabling MSLAB (and pool) is totally waste. That is a mis configuration. More and more on demand BB creation, when MSLAB pool is a bad sign. We have a WARN log for just one time as of now.. May be we should repeat this log at certain intervals. (Like for every 100th pool miss or so.) We should be able to turn MSLAB usage ON/OFF per table also. Now this is possible? Am not sure. These 2 things need to be checked and done in another jiras IMO.",Design-Des,yes,ab5970773ad187871270bb63f9a37cead4cf2f6a
HBASE,17338,comment_19,+1 on commit. Lets get up on this new basis. Nice cleanup  (Thanks too for doc edit).,Code-LQualCd,yes,ab5970773ad187871270bb63f9a37cead4cf2f6a
HBASE,17338,description,"We have only data size and heap overhead being tracked globally. Off heap memstore works with off heap backed MSLAB pool. But a cell, when added to memstore, not always getting copied to MSLAB. Append/Increment ops doing an upsert, dont use MSLAB. Also based on the Cell size, we sometimes avoid MSLAB copy. But now we track these cell data size also under the global memstore data size which indicated off heap size in case of off heap memstore. For global checks for flushes (against lower/upper watermark levels), we check this size against max off heap memstore size. We do check heap overhead against global heap memstore size (Defaults to 40% of xmx) But for such cells the data size also should be accounted under the heap overhead.",Design-Des,yes,ab5970773ad187871270bb63f9a37cead4cf2f6a
HBASE,17480,comment_2,+1 That is a nice lump of code removed. You remove Do we have an explicit test of the splitting path any more post this removal?,Code-DedCd,yes,bff7c4f1fda5517c469db7863706140e3c97e9e0
HBASE,17480,description,"HBASE-14551 moves the split region logic to the master-side. With the code in HBASE-14551, the split transaction code from region server side became un-used. There is no need to keep region_server-side split region code. We should remove them to avoid code duplication.",Code-DedCd,yes,bff7c4f1fda5517c469db7863706140e3c97e9e0
HBASE,18085,comment_6,Any chance for a micro benchmark tests Yu? Appreciate ur effort to consider all possible options :-),Test-LacTst,yes,d047cc9ecc7e0067c3b3aaeeb1600da8d774b211
HBASE,18085,comment_9,"Reading the code in I can see it uses a state variable up in the layer in which is declared volatile. So we ourselves using a boolean volatile will be better any way right? Pls check once whether I misread some code path,",Code-LQualCd,yes,d047cc9ecc7e0067c3b3aaeeb1600da8d774b211
HBASE,18085,comment_13,"Sorry, don't quite catch you sir, mind clarify? While writing the JMH case, I felt that using AtomicBoolean or volatile boolean we're actually re-implementing the tryLock logic. And I won't be surprised if JIT has any optimization for its native implementation (smile).",Code-DupCd,yes,d047cc9ecc7e0067c3b3aaeeb1600da8d774b211
HBASE,18085,comment_14,"In ur test method for tryLock, there is no logic other than just try lock and release. If that is been removed as dead code by compiler? One way to avoid that is using the return value. See BlackHole in JMH and its usage.. The diff in numbers that reported by JMH benchmark is so huge! The impl of tryLock is having a volatile read and all. So this huge diff in numbers looks strange no? That was my doubt.",Code-DedCd,yes,d047cc9ecc7e0067c3b3aaeeb1600da8d774b211
HBASE,18085,description,"Parallel purge in ObjectPool is meaningless and will cause contention issue since has synchronization (source code shown below) We observed threads blocking on the purge method while using offheap bucket cache, and we could easily reproduce this by testing the 100% cache hit case in bucket cache with enough reading threads. We propose to add a purgeLock and use tryLock to avoid parallel purge.",Code-MTCor,yes,d047cc9ecc7e0067c3b3aaeeb1600da8d774b211
HBASE,18092,comment_4,"Attaching another patch for master. I found a redundant piece of code related to the patch. The only difference between the previous and this patch is {Code} @@ -528,9 +542,7 @@ public class implements ReplicationListener { */ public void src) { LOG.info(""Done with the recovered queue "" + - if (src instanceof ReplicationSource) { - - } + {Code} Also attaching patches for branch-1 and branch-1.3 as they both have diverged a little bit.",Code-Oth,yes,7b40f4f3ec1cdf278bf416db66284d62c4e078e0
HBASE,18092,comment_13,"Just a nitpick, I think you merged the incorrect patch for master. I was looking at my latest master checkout and see that patch for master that went in is dated May 25, not yesterday. It's not a big deal in this case because the two patches have no functional difference, only that the newer patch got rid of a redundant piece of code.",Code-DedCd,yes,7b40f4f3ec1cdf278bf416db66284d62c4e078e0
HBASE,18092,description,Removing a peer does not clean up the associated metrics and state from walsById map in the,Code-LQualCd,yes,7b40f4f3ec1cdf278bf416db66284d62c4e078e0
HBASE,18092,summary,Removing a peer does not properly clean up the state and metrics,Code-LQualCd,yes,7b40f4f3ec1cdf278bf416db66284d62c4e078e0
HBASE,18180,comment_1,Have a look at TableOutputFormat inside Connection leak problem has already been addressed in this package. Code sample for reference.,Code-LQualCd,yes,ce1ce728c6daee9e294bf2915e91faaa73428f3d
HBASE,18180,summary,Possible connection leak while closing BufferedMutator in TableOutputFormat,Code-LQualCd,yes,ce1ce728c6daee9e294bf2915e91faaa73428f3d
HBASE,18646,comment_1,"If the config is specific to log roll, please reflect this in the name of config.",Code-LQualCd,yes,19bb4ef487407cdf59a3f3c5bbef6dd8b917b682
HBASE,18646,comment_4,Isn't the above default too short for a large cluster ? Please remove commented out code from patch.,Code-DedCd,yes,19bb4ef487407cdf59a3f3c5bbef6dd8b917b682
HBASE,18646,description,The default procedure timeout of 60 sec and pool size (1) may be not optimal for large deployements,Design-Des,yes,19bb4ef487407cdf59a3f3c5bbef6dd8b917b682
HBASE,19073,comment_2,"So the only test passing in QA is But it's weird, this is second time am seeing it failing in a precommit, and the last patch was completely unrelated. It's not even in the flaky list. Let me dig.",Test-FlakyTst,no,
HBASE,19073,description,"- Remove the configuration - Keep following interface since they nicely separate ZK based implementation: ProcedureMemberRpcs - Replace CSM (interface) + BCSM (unnecessary middle hierarchy) with single CSM interface. - Don't pass whole CSM object around (with server in it which gives acess to pretty much everything), only pass the relevant dependencies. Discussion thread on dev@ mailing list.",Design-Des,yes,dd70cc308158c435c6d8ec027e2435a29be4326b
HBASE,19073,summary,Cleanup,Code-LQualCd,yes,dd70cc308158c435c6d8ec027e2435a29be4326b
HBASE,19183,description,Currently the modules hbase-checkstyle and hbase-error-prone define the groupId redundantly. Remove the groupId from these POMs.,Code-LQualCd,yes,d4e3f902e6ba5b747295ca6053f34badd4018175
HBASE,19183,summary,Removed redundant groupId from Maven modules,Code-LQualCd,yes,d4e3f902e6ba5b747295ca6053f34badd4018175
HBASE,19241,comment_5,"+1 after fix the checkstyle warnings. Meanwhile, please add one line change change to trigger hbase-server test. Thanks.",Code-LQualCd,yes,5c312667ed9b9b3bbd870c7fcf26591479da000d
HBASE,19241,comment_6,Fix checkstyle and javadoc warnings. Add one line change in hbase-server module to trigger the UTs.,Code-LQualCd,yes,5c312667ed9b9b3bbd870c7fcf26591479da000d
HBASE,19241,summary,Improve javadoc for AsyncAdmin and cleanup warnings for the implementation classes,Code-LQualCd,yes,5c312667ed9b9b3bbd870c7fcf26591479da000d
HBASE,19373,description,Fix the remaining Checkstyle error regarding line length in the *hbase-annotations* module.,Code-LQualCd,yes,07b193ae4f36345c4073023e0220b3e147c61884
HBASE,19373,summary,Fix Checkstyle error in hbase-annotations,Code-LQualCd,yes,07b193ae4f36345c4073023e0220b3e147c61884
HBASE,19478,description,Currently issues one Get per WAL file: This is rather inefficient considering the number of WAL files in production can get quite large. We should use multi-get to reduce the number of calls to backup table (which normally resides on another server).,Code-SlAlg,yes,cafd4e4ad76f45be912edc9d5021f872de94fd5c
HBASE,19531,summary,Remove needless volatile declaration,Code-LQualCd,yes,9d0c7c6dfbcba0907cbbc2244eac570fcc4d58a5
IMPALA,1013,description,This function is very simple and goes through the HS2 interface to retrieve a few functions. I believe this used to be instantaneously and now takes hundreds of seconds making this likely unusable.,Code-SlAlg,yes,c096d7688102e0570d2461ccc9745abded1b5ae6
IMPALA,1013,summary,is unreasonably slow,Code-SlAlg,yes,c096d7688102e0570d2461ccc9745abded1b5ae6
IMPALA,1022,description,"rows_read < rows_in_file) { We should detect the case where doesn't actually equal the number of rows in the file. If abort_on_error is true, this should fail the query, otherwise we should log something via scan_node_-Such handling did not exist before. Need to decide whether we will read at most as many rows as the metadata or continue reading until there are no more rows in the file. We will need to add tests with parquet files whose metadata is not correct.",Test-LacTst,yes,fe0646f76bf35b525f8f63948a7d3baa481e5bc0
IMPALA,1120,description,"We should update our code that fetches and updates column stats to use the new bulk APIs introduced in Hive .13. Instead of fetching a single column at a time, we can now fetch stats for a list of column names.",Architecture-ObsTech,yes,7542d719c941576d967876a89421dfe0d1e51524
IMPALA,1147,description,The view compatibility tests are disabled and need to be updated to run against Hive .13.,Test-LacTst,yes,2a59029c2cbac2176df424dfcd01f6653b65b955
IMPALA,1147,summary,View compatibility tests need to be updated to run against Hive .13,Test-LacTst,yes,2a59029c2cbac2176df424dfcd01f6653b65b955
IMPALA,1290,description,If an AnalyticEvalNode is the root of a plan tree it should have results ready when Open() returns so that clients do not time out once the query is in a ready state and expect to be able to fetch results. This only happens for unpartitioned analytic evaluation. Should test this in,Test-LacTst,yes,f9b60bce43164d3a3598127ae078bc4f33640720
IMPALA,1414,description,"The following query on a relatively small input takes ~70s with codegen and ~35 minutes without codegen, which seems unreasonably long. We should make sure we're not doing anything crazy in the non-codegen eval path. Executing with codegen: The query profile when running without codegen is attached. I also used perf (linux profiling tool) to collect a profile (sampled stacks) and printed the aggregated callstacks to the attached file The summary without codegen. Most of the time (28mins) is spent in the hash joins:",Code-SlAlg,no,
IMPALA,1414,summary,Slow query without codegen,Code-SlAlg,no,
IMPALA,1430,description,"Currently codegen is disabled for the entire aggregation operator if a single aggregate function can't be codegen'd. We should address this by making it so all aggregate functions can be codegen'd, including UDAs. For UDAs in .so's, the codegen'd function will call into the UDA library. This also affects aggregation operator on timestamp. This perf hit can be especially bad for COMPUTE STATS which is heavily CPU bound on the aggregation and because there is no easy way to exclude the TIMESTAMP columns when computing the column stats (i.e., there is no simple workaround). Even if the portions involving TIMESTAMP cannot be codegen'd it would still be worthwhile to come up with a workaround where codegen for the other types is still enabled. *Workaround* If you are experiencing very slow COMPUTE STATS due to this issue, then you may be able to temporarily ALTER the TIMESTAMP columns to STRING or INT type before running COMPUTE STATS. After the command completed, the columns can be altered back to TIMESTAMP. Note the workaround is only apply to text data, not parquet data. parquet require compatibles data type. TIMESTAMP is INT96, it's not compatible with STRING or BIGINT.",Design-Des,yes,d7246d64c777384f29fe0f824ee0036b58e8aa2d
IMPALA,1596,description,Some the builtin decimal functions assume they can always use the val16 union field of a DecimalType. This isn't actually a valid assumption though. We should only initialize and use the smaller fields (val4/val8) for performance if possible.,Code-LQualCd,yes,31e95e6b440f0317acc8bf3d1ac4d56c8696c376
IMPALA,1598,description,"Some error conditions cause a high volume of output that gets sent to the users, particularly if each backend produces several messages that are aggregated by the coordinator. An example is the Parquet error message when a file spans several blocks: you get one warning per file, all of which are communicating roughly the same thing. Unfortunately we can't just do simple string-matching to de-duplicate the messages, because they're all slightly different (filename changes). We should add an error code to each user-facing message, and use that as the unique key with which to aggregate the messages when there are lots of them.",Design-Des,yes,b582cdc22b757e929016beed795b9a5d4eff6f59
IMPALA,1651,description,"Currently, CREATE TABLE LIKE 'blindly' copies the source table, including the table's and partitions' parameter maps. The parameter maps may contain information about caching directives that do not apply to the new table. I can see two acceptable behaviors (choose one) if the source table is cached: 1. the new table is also cached; we need to issue new caching directives and store them in the params of the new table as appropriate 2. the new table is not cached and must be cached explicitly",Design-Des,yes,aed3505c8d17f759be4f73593f6aa290df384cfa
IMPALA,1691,comment_1,We should get rid of the cached {{Partition}} object inside {{HdfsPartition}}. I'm pretty sure we can reconstruct it from the partition itself when needed (i.e. when sending updates to the HMS).,Design-Des,yes,8c30332237fb6d5a946b80e5fed923e80d064d15
IMPALA,1691,comment_2,I just looked at a catalogd instance with a large table loaded in a profiler. This seems to bear out the idea that the arrays of {{FieldSchema}} in the take a lot of memory. take up about 1.1GB of heap even with a relatively small catalog.,Design-Des,yes,8c30332237fb6d5a946b80e5fed923e80d064d15
IMPALA,1691,description,"It used to be the case that the entire Catalogd's memory usage won't go over 1~2GB. However, a single table with 1000 columns and 8000 partitions used up 1.3GB of memory in the catalogd. The attached script created such a table. To see the memory usage, start catalogd without background loading. Observe the initial memory usage. Then run ""describe"" to load the table and observe the memory usage again.",Design-Des,yes,8c30332237fb6d5a946b80e5fed923e80d064d15
IMPALA,1691,summary,Excessive Memory Usage in Catalogd (without stats),Design-Des,yes,8c30332237fb6d5a946b80e5fed923e80d064d15
IMPALA,1929,description,"While running the mem leak test with 4 concurrent clients using the release bits an impalad crashed at with the following stack trace. This can happen in case of RIGHT_OUTER, RIGHT_ANTI and FULL_OUTER joins when the hash table of the partition is NULL. The code has a dcheck that the hash_tbl is not null, which we didn't hit because we were using the release bits.",Code-LQualCd,yes,05b800e56703f7c4e94914dcf4bee7f955e8a647
IMPALA,1934,description,"When LDAP authentication is enabled in Impala, impala-shell always prompts for a password when trying to connect to an Impala daemon, which makes it very hard to use impala-shell in a shell script (where to store the password?) Please make impala-shell support reading password from a command line parameter or from a password file, just like what beeline does.",Design-Des,yes,cca964c3c6d6744b65741c8875ada94cad876042
IMPALA,1963,comment_2,"For my current project, I only really need it to handle the Z properly. Anything else (other timezones, full support) would be a bonus. In terms of Tableau/Impala usability - this problem will come up for anyone with a format that doesn't convert automatically, so the more coverage we get the better. However, in terms of release schedules, I would love to have time zones or at least Zulu time working sooner.",Design-Des,yes,b0f7dabab3d5ee059e39c63b1f479cb29929e747
IMPALA,1963,description,"We have several data sources that return dates in ISO-8601 format. Following IMPALA-648, we are closer to having these convertable to timestamp without effort, but time zones are still not supported: Neither format with a time zone specifier will parse even though they are valid based on my understanding of ISO-8601 and For now we have worked around this by converting with substring to cut off the time zone identifier, but I'm not sure which time zone the dates will be interpreted in. If I cut off the Z, will my dates be parsed in server local time? Ideally, we should be able to cast dates with time zone specifiers to timestamps with cast easily.",Code-LQualCd,yes,3a962e46c5b31dbbcfe4029fdd17c50e09a4d4df
IMPALA,2068,description,"It is currently difficult for clients connected via load balancers to know which coordinator they're connected to. The TOpenSessionResp has a map of configuration settings which are returned, and we can return the connected coordinator in that map. Clients such as Hue can use that information to find the coordinator and thus the debug web UI pages for submitted queries over that session.",Design-Des,yes,938158e1c3a5400dda8a65ee75c4969bdcdaf0b5
IMPALA,2076,description,"*Problem Statement* The ExecSummary does not report the time spent on the network. The time reported on the EXCHANGE node is misleading and does not count network time. This makes it impossible to identify the performance bottleneck. *Cause* The ExecSummary reports the non-waiting time spent in the EXCHANGE as the network time. Obviously, the active time spent in the EXCHANGE is zero (or close to zero). It should be replaced by the corresponding Data Stream Sender total time instead. *Workaround* Examine the corresponding time in the Data Stream Sender instead.",Design-Des,yes,4f18c701ba146130d82cae36455977d7ea4c57d0
IMPALA,2244,description,"This function now loops through each scan range and when there are many, is noticeably slow. For example, an EXPLAIN on a query against 1M blocks and 1000K partitions went from around 450 ms to 700 ms with this change.",Code-SlAlg,yes,750786fee262e5c013026243e07bf4dbab17671c
IMPALA,2244,summary,HdfsScanNode.java computeNumNodes() can be slow,Code-SlAlg,yes,750786fee262e5c013026243e07bf4dbab17671c
IMPALA,2295,description,doesn't deep copy collections. This leaves the destination Tuple with pointers into potentially invalid memory.,Design-Des,yes,235a8d08da9fbc1aaa170c952c87f078623cc506
IMPALA,231,comment_0,We have to mark local var using "DeleteLocalRef" when the local var is done. This will help reduce mem pressure.,Code-LQualCd,yes,7aadac236de0cdd7e8fb88d1dcc2adad2662caec
IMPALA,231,description,"Impala's HBase scan is very slow. For scanning 40000 rows from an colocated HBase table, it took ~5.7sec. The query is: select count(*) from (select * from hbasetbl limit 40000); Majority of the time is spent inside I've done some experiment to figure out where we spent the time by adding a few more timers. Here's the runtime profile: HBASE_SCAN_NODE (id=0):(5s714ms 99.98%) - BytesRead: 4.26 MB - 483.594us - 5s710ms - MemoryUsed: 0.00 - MyOwnTimer1: 1s387ms <-- We should trim this time. - MyOwnTimer2: 2s798ms <-- - MyOwnTimer3: 688.179ms - MyOwnTimer4: 0ns - MyOwnTimer5: 0ns - NumDisksAccessed: 0 - 5.14 MB/sec - RowsReturned: 40.00K (40000) - RowsReturnedRate: 7.00 K/sec - ScanRangesComplete: 0 - 0 - 0ns - 0ns - 0ns - 0ns - 0 - 827.604ms <-- we spent only 800ms on fetching from HBase - 775.05 KB/sec I've attached the code with more timers in the attached file When I increase the limit to 80000, the perf is much worse. Here's the profile: HBASE_SCAN_NODE (id=0):(23s027ms 99.99%) - BytesRead: 8.51 MB - 249.40us - 23s018ms - MemoryUsed: 0.00 - MyOwnTimer1: 5s680ms - MyOwnTimer2: 11s401ms - MyOwnTimer3: 2s829ms - MyOwnTimer4: 0ns - MyOwnTimer5: 0ns - NumDisksAccessed: 0 - 2.76 MB/sec - RowsReturned: 80.00K (80000) - RowsReturnedRate: 3.47 K/sec - ScanRangesComplete: 0 - 0 - 0ns - 0ns - 0ns - 0ns - 0 - 3s085ms - 376.52 KB/sec I didn't see any disk activity. So, I dont' think HBase is going to disk. Also, the read time is only 3sec. The big chuck of time is spent within the block of MyOwnTimer1 and MyOwnTimer2. We spent 5x more time there even though we only scan 2x more rows.",Code-SlAlg,yes,7aadac236de0cdd7e8fb88d1dcc2adad2662caec
IMPALA,231,summary,Impala HBase scan is very slow,Code-SlAlg,yes,7aadac236de0cdd7e8fb88d1dcc2adad2662caec
IMPALA,2341,description,"Ideally, we should make this query work. Alternatively, the error message should be improved. Stack:",Code-LQualCd,yes,84e2c043a4a7c0bbf52bd0786924e2dd86d7c17e
IMPALA,2435,description,"While I was reviewing Dimitris's IMPALA-2369 patch, I realized that in AddBatch() we call Run::Init() without checking the return status of it. That means that Dimitris's patch probably won't do much good :) At Instead of creating a different patch, I will let Dimitris add it to his IMPALA-2369 patch.",Design-Des,yes,7d338af63806fc59b7c3c8edf14a1e60940c7c0c
IMPALA,2457,description,"According to the math behind the PERCENT_RANK() value, a row group containing a single row yields a division by zero error and therefore Impala returns NaN in this case. In this example, there is only 1 'Reptile' row and that line in the result set has a NaN: Most RDBMS examples on the web use data with multiple rows per group, so they don't illustrate what's supposed to happen in this case. But I suspect the right return value in this case might be zero. Cf. ""The first row in any set has a PERCENT_RANK of 0."" That statement suggests the first row (and any tied rows I presume) could be special-cased and no calculation performed. Cf. The analytic example doesn't show all of the relevant output, but there are two DEPARTMENT_IDs in the output that have only one relevant row, and in both those cases the PERCENT_RANK result is shown as 0. I didn't find any PERCENT_RANK() in MySQL to try. I don't have a PostgreSQL instance handy.",Code-LQualCd,yes,f06497e1d61d32b9fffd99a580f56e9a14be3f40
IMPALA,2632,description,"Currently impalad needs external LLVM IR files to be compiled separately from the binary and distributed with the binary. This slows down builds, make it easy for the code to get out of sync in dev environments, and complicates distribution. Instead, we could simply link the IR data into the binary file, so impalad is more self-contained. These IR files are also loaded from disk by codegen for each fragment. Linking the IR into the binary would avoid this step, saving some overhead. Could also address IMPALA-799 (.bc file extension instead of .ll extension) at same time.",Design-Des,yes,47f7f687b9d7090e2222af4b67d9875c9e26ddde
IMPALA,2642,description,"I just noticed this while reading the statestore code: {{OfferUpdate()}} takes if the update queue is full, but in one place that lock has already been taken by the caller: It's not as scary as it sounds, because if the update queue has > 10000 entries there's something wrong anyway, but we should fix this.",Code-MTCor,yes,f97634fcf9a2e48d307f19595e70bdcee9c1980e
IMPALA,2707,description,"For each input row the aggregation node uses HashTable::Find() followed by HashTable::Insert() if the grouping key isn't already present in the table. Both of these methods probe the hash table to find the same bucket. If we added a FindOrInsert() method to the hash table that returned a modifiable iterator pointing to the bucket, we could save a significant number of hash table probes. There is already a TODO in the code for this, so I'm creating a JIRA to track the issue. This could speed up aggregations with large output size significantly, e.g. TPC-H query 13 (see IMPALA-2470).",Code-SlAlg,yes,42e92abfe017bb074e4bf24b630d646b49e5369e
IMPALA,2707,summary,Add FindOrInsert method to hash table to avoid unnecessary probe in aggregation,Design-Des,yes,42e92abfe017bb074e4bf24b630d646b49e5369e
IMPALA,3017,description,I saw a private build where an impalad appears to get OOM-killed while executing the below tests. I believe what is probably happening is that a bunch of large allocations happen at the same time.,Test-ExpTst,yes,d2e8881065986ae2a74da9d2dd246e7f92393922
IMPALA,3077,description,"Because of a quirk in the runtime filters implementation, we currently have to disable them when a join spills. The only reason this is necessary is that the filters are constructed as part of the hash table build. But there is no reason that we need to construct the filters at that point: we could instead construct the filters when doing initial processing of the build input, at which point we see all build-side input rows regardless of whether they are spilled or not. This might actually perform better since it would move some of the CPU work from the CPU-intensive hash table build to the less CPU-intensive construction of the input stream.",Design-Des,yes,c14a6f11dfa3a3bd66d4868bd6e20f81ba822420
IMPALA,3103,comment_1,The following fix improves serialisation efficiency by 20x (!),Code-SlAlg,yes,0a6954de74eec4cca77d1172cff7f206fbfb2c7e
IMPALA,3103,description,"{{TBloomFilters}} have a 'directory' structure that is a list of individual buckets (buckets are about 64k wide). The total size of the directory can be 1MB or even much more. That leads to a lot of buckets, and very inefficient deserialisation as each bucket has to be allocated on the heap. Instead, the {{TBloomFilter}} representation should use one contiguous string (like the real {{BloomFilter}} does, so that it can be allocated with a single operation (and deserialized with a single copy).",Code-SlAlg,yes,0a6954de74eec4cca77d1172cff7f206fbfb2c7e
IMPALA,3103,summary,Improve efficiency of BloomFilter Thrift serialisation,Code-SlAlg,yes,0a6954de74eec4cca77d1172cff7f206fbfb2c7e
IMPALA,321,description,In the summary node in the profile the info strings for start / end times are specified with second granularity. It would be nice to have them specify milliseconds.,Design-Des,yes,11decfa48a3578d19b50513c3a1a8daae7721ae4
IMPALA,322,description,"It would be nice to get able to get information about errors the query encountered, like corruptions, missing block metadata, integer overflows, and anything you guys think would be interesting, in the profile so it's all centralized.",Design-Des,yes,d730071dde3193dfacbeb351b8f406e4a632e8dd
IMPALA,3276,description,PrepareForRead() allows callers to use it in two different modes. In one of the modes it does not report pin failures. The code was written assuming that it will have reservations. This can result it hitting a DCHECK or going down some strange code paths:,Code-LQualCd,yes,36c294b55e64b6b9dd1c0fca30205a05db24b120
IMPALA,330,description,"Web page requests hold for the duration of the callbacks that render the page. This isn't terrible for short-lived requests, but anything that takes a while to render will block other requests. I tried the obvious thing, and Mongoose started behaving very weirdly (rendering the wrong page, rejecting a lot of concurrent requests).",Code-MTCor,yes,1e88e6d0c07e37a30a3100dadbe36e868c530edc
IMPALA,3329,description,"I believe the {{impalad}} log rotation policy is causing logs to rotate out that we don't have access to. {{impalad}} defaults to keeping 10 log files in the log directory. This means if {{impalad}} restarts 11 times, the first logs will be rotated out. Many of the custom cluster tests restart {{impalad}}, though. This means {{impalad}} logs in the custom_cluster logs subdirectory are missing. To prove this is happening as a result of the log rotation policy, and it's not our inability to properly collect logs, I ran locally, and to get timestamps to show more about when tests were being run, I used this: (0) This gets us timestamps when there otherwise wouldn't be any. Here's the time range for our custom cluster tests: The earliest timetstamp log files are for 20:58. Contrast this to the end-to-end tests' log directory, which only has a handful of logs and span the 2 hours it took to run the test. The quick fix is to set the custom cluster tests to run with a limitless log rotation policy. The more detailed fix is to probably plumb things through such that we don't ever rotate logs. This is safe in Jenkins since we deleted the logs directory before every run (in code): we won't have a workspace accumulating logs. (0) Yup, you need {{gawk}} for {{strftime}} which {{mawk}}, the default {{awk}} on at least Ubuntu, doesn't have.",Design-Des,yes,50851f8775070fb0c14a0415a1e41ea71ac1f5a1
IMPALA,3344,description,"The sorter code does not have its internal invariants well-documented and can be tricky to understand when working on it. E.g. what are the valid states for merged versus initial runs, how are end-of-block cases handled. This issue is to clean up the sorter code while documenting important invariants as preparatory work for porting the sorter to the new buffer pool.",Code-LQualCd,yes,37ec25396f2964c386655ce3408b32a73f71edf6
IMPALA,3344,summary,Simplify and document invariants in Sorter,Code-LQualCd,yes,37ec25396f2964c386655ce3408b32a73f71edf6
IMPALA,3548,description,"Currently, the FE generates a number of runtime filters and assigns them to plan nodes without taking the value of RUNTIME_FILTER_MODE into account. In the backend, the filters are removed from the exec nodes based on the target node types (local or remote) and the value of the RUNTIME_FILTER_MODE option. This may cause some confusion to users because they may see runtime filters in the output of explain that aren't applied when the query is executed. This operation should be performed in the FE.",Design-Des,yes,00fd8388c37e1b3207dddf564463b7eeafe3f887
IMPALA,3652,comment_1,"The required fix is for Reset() to take a RowBatch as an argument, to which memory referenced by previously returned rows can be returned. The logic for Reset() attaching memory needs to be similar to the handling of ReachedLimit(), e.g. this example in NLJNode::GetNext():",Design-Des,yes,c28bc3e4f3900bfd0b14084ca19e51b36ea4dca7
IMPALA,3652,description,"There is a tricky corner case in our resource transfer model with subplans and limits. The problem is that the limit in the subplan may mean that the exec node is reset before it has returned its full output. The resource transfer logic generally attaches resources to batches at specific points in the output, e.g. end of partition, end of block, so it's possible that batches returned before the Reset() may reference resources that have not yet been transferred. It's unclear if we test this scenario consistently or if it's always handled correctly. One example is this query, reported in IMPALA-5456:",Design-Des,yes,c28bc3e4f3900bfd0b14084ca19e51b36ea4dca7
IMPALA,3671,description,"The immediate motivation for this is to enable better testing of graceful failures when spilling is disabled (e.g. IMPALA-3670). Currently we only have control over this via startup options, so we have to implement these as custom cluster tests, but there's no reason in principle we need to start a fresh cluster. The idea would be to add a query option 'scratch_limit' or similar, that limits the amount of scratch directory space that can be used. This would be useful to prevent runaway queries or to prevent queries from spilling when that is not desired.",Design-Des,yes,9313dcdb830b0cd24479ca892988d17defc9ca19
THRIFT,1003,description,"attached patch contains following changes: * Added Apache headers to c/h files * Use gtester for running tests. We don't need -wrapper script anymore * Use one-line macros G_DEFINE_TYPE instead of 15-line class definition * Keep formatting closer to glib-like style (one line class definition macroses/remove trailing spaces) Given changes are mostly fixing low hanging fruits. It does not change any logic/api. There are more chages needed, such as * using CLASS_TYPE_new functions instead of * stop using _set_property (aka reflection) in constructors * check more careful about _ref and _unref handling but this requires more careful refactoring so it will be later in a separate patch.",Code-LQualCd,yes,c101092ea742e1252207b6e8f680bf392292c916
THRIFT,1003,summary,Polishing c_glib code,Code-LQualCd,yes,c101092ea742e1252207b6e8f680bf392292c916
THRIFT,1055,description,"resulted in TFramedTransport being improved so it wasn't necessary to disable Nagle. For the case of TServerSocket and TSocket, a simple server case, performance is still very poor, ~30 msg/sec vs. ~11k msg/sec with NoDelay = true. Java and cpp disable nagle in their TSocket and TServerSocket implementations and performance is MUCH improved with nagle disabled for csharp so I'm proposing that csharp should also disable nagle.",Design-Des,yes,2d9dfdb88e58ab5b961726a5506a4f3e0653b807
THRIFT,1065,comment_1,remove some misplaced code (probably copied from another implementation?),Code-DedCd,yes,b3b07d6de4fd673fd8acd1484daa8bf9002d91cc
THRIFT,1065,comment_3,"apparently the java server is not encoding undeclared exceptions ""properly"" (it could be my impl., please see patch), but it works fine on C++ Incoming content: [cpp] Outgoing content: is a [java] Outgoing content: Error:This is a",Design-Des,yes,76d55f635784aa9dfae8ce52ce3eb49ba7f90a40
THRIFT,1065,summary,Unexpected exceptions not proper handled on JS,Code-LQualCd,yes,a8738b5560db8216c06d0a8cea116b7f29255e8e
THRIFT,1100,description,"The python TSSLSocket.py module has TSSLSocket and TSSLServerSocket for outbound and inbound SSL connection wrapping. This ticket is for a patch that makes several improvements: * adds Apache license at top of file * for outbound sockets, SSL certificate validation is now performed by default ** but may be disabled with validate=False in the constructor ** instructs python's ssl library to perform CERT_REQUIRED validation of the certificate ** also checks to make sure the certificate's {{commonName}} matches the hostname we tried to connect to ** raises when the certificate fails validation - tested using google's www.gmail.com (doesnt match) versus mail.google.com (matched cert commonName) ** puts a copy of the peer certificate in self.peercert, regardless of validation status ** sets a public boolean self.is_valid member variable to indicate whether the certificate was validated or not * adds a configurable server certificate file, as a constructor argument {{certfile}} ** allows runtime changing of server cert with setCertfile() on the server, that changes the certfile used in subsequent ssl_wrap() calls ** exposes a class-level variable SSL_PROTOCOL to let the user select ssl.PROTOCOL_TLSv1 or other versions of SSL, instead of hard-coding TLSv1. Defaults to TLSv1 though. * removes unnecessary sys.path modification * adds lots of docstrings In a somewhat unrelated change, this patch changes two lines in TSocket.py where self.handle is compared to None using {{!=}} instead of: {{is not}}.",Code-LQualCd,yes,5040911bfab39b5c9f2a0d715cea0ee9012f7450
THRIFT,1103,comment_2,"Thanks for comitting THRIFT-1094, I'll update the patch this evening (and attach as _v2) to include testing TZlibTransport wrapping in the code (as a cmdline --zlib argument to both scripts). FYI, currently the hudson(jenkins) build seems to be failing on the javascript jslint tasks, stopped the build tests from progressing past the 'test/js' directory...",Test-LacTst,yes,1606659171d9ee8885d5806d6030ec39399b3b08
THRIFT,1103,comment_3,"I updated the test suite to include running every valid combination of server, protocol and wrapping transports (both ssl and zlib). For python2.4, this is 30 combinations and runs in about 24 seconds. For python2.7, there is an extra server type (TProcessPool which uses the multiprocessing module) and the SSL transport (unavailable in py2.4), whichadds up to 66 combinations of tests, running in ~95 seconds. The 4 nested for-loops significantly expands the code test coverage. In addition to everything in the _v1 of this patch, the _v2 version also has: Updated test code: * added testing of TSSLServer, an alternate socket transport * added testing of TZlibTransport, a wrapping transport * added a self-signed cert in with a cautionary .readme to allow testing of the TSSLServerSocket (it needs a certificate file) * fixed -q (quiet) and -v (verbose) options to to lower and raise the verbosity Fixed two problems in and one enhancement: * fixed confusing parameters to both client and server constructors, removing the overly ornate \*args and \*\*kwargs which made the constructor behave poorly when used with just (host,port) as arguments. The constructors better match the TSocket and TServerSocket constructor parameters now. * fixed logic in TSSLServerSocket parameter checking, if validate=True and ca_certs=None, now it raises an exception like the docstring claims it should. * made TSSLServerSocket more robust on failed SSL handshake by closing socket connection and returning None from accept() call, which is better than terminating the entire server in some cases I will attach the _v2 patch in a moment.",Code-LQualCd,yes,1606659171d9ee8885d5806d6030ec39399b3b08
THRIFT,1103,description,"New implementation of zlib compressed transport for python. The attached patch provides a zlib compressed transport wrapper for python. It is similar to the TFramedTransport, in that it wraps another transport, implementing the data compression as a transformation layer on top of the underlying transport that it wraps. The compression level is configurable in the constructor, from 0 (none) to 9 (best) and defaults to 9 for best compression. The way this works is that every write() to the transport appends more data to the internal cStringIO write buffer. When the transport's flush() method is called, the buffered bytes are then passed to a zlib Compressor object and flush()ed with zlib.Z_SYNC_FLUSH. Because the thrift API calls the transport's flush() after writeMessageEnd(), this means very small thrift RPC calls don't get compressed well. This transport works best on thrift protocols where the payload contains strings longer than 10 characters. As with all data compression, the more redundancy in the uncompressed input, the greater the resulting compression. The TZlibTransport class also implements some basic statistics that track the number of raw bytes written and read, versus the decompressed equivalent. The getCompRatio() method returns a tuple of where ratio is computed using: (So 10 compression is 0.10, meaning smaller numbers are better.) The getCompSavings() method returns the actual number of which might be negative when the compression of non-compressible data ends up expanding the data. So hopefully, anyone who uses this transport will be able to tell whether the compression is saving bandwidth or not. I will add the patch in a few minutes. I haven't tested this against the C++ TZlibTransport, only against itself.",Test-LacTst,yes,1606659171d9ee8885d5806d6030ec39399b3b08
THRIFT,1176,description,Below I added the var qualifier on the declare field function. This is fairly trivial and fixes the global scope leak.,Code-LQualCd,yes,5860f8850e049a22e69022697a899958aa00b534
THRIFT,1176,summary,Thrift compiler global leakage js,Code-LQualCd,yes,5860f8850e049a22e69022697a899958aa00b534
THRIFT,1199,description,"For example, in the following union it would be nice to be able to do something like {{boolean test = as an alternative to {{boolean test = ==",Code-LQualCd,yes,63c2d378c53ab7340466495b49451e68a7411c6c
THRIFT,1199,summary,Union structs should have generated methods to test whether a specific field is currently set,Test-LCvg,yes,63c2d378c53ab7340466495b49451e68a7411c6c
THRIFT,1217,comment_1,"I really like to have Windows port within next Thrift release. The suggestion by [David and to use APR on Windows and do minimal changes on the current implementation is a key factor to bring this up and running. However, this patch depends on pthreads for windows and as mentioned on the dev mailing list or within Jira = Is APR feasible for your? Do you really need thet libevent version? Most GNU/Linux distro still provide 1.4.x and I would really like to keep capability to build and use thrift without lot of extra versions of libraries not within a standard distribution.",Architecture-ObsTech,yes,30aae0ca877c9f5863ff881b29edc6a38df9d85a
THRIFT,1217,comment_2,"@Roger: to avoid some confusion: - the attached patch does not depend on pthread. The suggestion here is merely to use evutil_socketpair instead of pipe. - evutil_socketpair is part of libevent 1.4 on ubuntu 10.10, although I did not test it. All the other changes related to win32 in the attached patch attached are not to take literally (Winsock2.h, static cast...). About THRIFT-1031: - my understanding is that THRIFT-1031 does not support async/libevent server on Windows (??) - libevent+thrift server seems VERY fast on win, and is needed for my project (blocking server won't do) - APR was not strictly necessary for the Non-Blocking server win32 port (i.e. although I tried the THRIFT-1031 patch and observed it made the port a bit cleaner, so I would not refrain from adding APR to thrift-C++ I decided after reviewing the win32 patches, it was better to open a separate issue, since the change here is atomic and should bare little consequences on linux. I was hoping that (naively) the win32 delta would get smaller by using the compatible call. All that said, I'd be really happy to contribute to THRIFT-1031 for it to go through. For this to happen, I would hope to have access to a shared implementation on top of 0.6.1 (the way I provided it on github), so I can test it regularly. Let me know if I can contribute somehow! As a conclusion, I think this patch is somewhat unrelated to THRIFT-1031, but it will help a future port of the NB server on win32 (assuming also someone will replace the pthread dependency by boost, which does not seem trivial at all).",Test-LacTst,yes,30aae0ca877c9f5863ff881b29edc6a38df9d85a
THRIFT,1217,description,"As part of an effort to use the Non-Blocking server (using libevent) on Windows, it was necessary to remove the use of ""pipe"" for the notification mechanism to signal end-of-task. We propose to use evutil_socketpair instead (tested with libevent 2.0.12). Patch included. Please see for more details.",Design-Des,yes,30aae0ca877c9f5863ff881b29edc6a38df9d85a
THRIFT,1231,description,"TAsyncChannel.h includes TTransportUtils.h, but doesn't really need/use it.",Code-DedCd,yes,fd39193aa00d2098184b452bd955bd60ae39f86d
THRIFT,1231,summary,Remove bogus include,Code-LQualCd,yes,fd39193aa00d2098184b452bd955bd60ae39f86d
THRIFT,1241,comment_0,"Thanks Darius, initial work looks great. Will want to change from namespace53 to just namespace since php5.3+ will have this option available and php5.4 is in rc right now so no sense in defining a specific version in the option. Can you attach this as a patch with asf inclusion please.",Design-Des,yes,de8d1857e8492f8d25abfb11a68ba9c90a49d99a
THRIFT,1241,description,Patch is based mainly on but some more improvements: namespace can be specified with dots '.' like for every other language for example: namespace php com.onego.thrift would generate in php file namespace com\onego\thrift to generate php files with namespaces use: thrift --gen php:namespace53 example.thrift,Code-LQualCd,yes,de8d1857e8492f8d25abfb11a68ba9c90a49d99a
THRIFT,1243,comment_2,"Saw this a bit late but I'm concerned about this change. With this change, there is no way for implementors of to signal to the callers that something unexpected happened or any other error conditions. Just because the current implementation ignores the return values doesn't mean they're not useful. Is the recommendation now to throw a TException instead?",Design-Des,yes,08077bf9d8c6c212f5ff384c94423b6f76892358
THRIFT,1243,comment_3,"@diwaker: Thanks for your comments, I share your concerns and am working currently on a patch for 0.8 that will use exception handling, and more generally handle/propagate libevent errors, log errors... For example I'll propose to remove 'abort' calls in TEvhttpServer.cpp, and replace them by exceptions. Also one significant issue I plan to cover, is to avoid propagating exceptions to libevent (which is C code): see THRIFT-1222 for example.",Design-Des,yes,08077bf9d8c6c212f5ff384c94423b6f76892358
THRIFT,1243,comment_4,"Included is a patch on 0.8 that adds more error handling to the libevent based server. It also fixes 2 minor bugs (handle failure of evhttp_bind_socket and prevent exceptions to reach libevent in As far as I can tell, the question of @diwaker is still valid, there is no good way currently to have the callback of to be passed the exact failure. However I don't think returning a bool helps, because libevent is the one asynchronously calling, so the error will have to be handled by the client *within* the callback. This callback will invariably throw a when a disconnect or a 404 happens. I added to this patch the ability to log such failures to stderr, but it would take more changes (compiler?) to actually pass the error to the client, so that a call will throw the correct error (and *not* EOF). Finally I removed all abort calls, trying to replace them by appropriate exceptions instead. Thanks!",Code-LQualCd,yes,08077bf9d8c6c212f5ff384c94423b6f76892358
THRIFT,1243,description,"[Context: This patch is part of a larger patch to use thriftnb on Windows/VisualC++. See for more details.] When compiling using Visual C++ 2010, the compiler chokes on casting some callbacks bool(*)() to void(*)(). Although probably valid and supported by gcc, this is further complicated by the fact those casting seem unnecessary: for each of the callbacks returning bool in TAsyncChannel.h: 1. the returned value is never checked 2. they always return true Attached is a trivial patch based on 0.7.0, tested on Ubuntu 11.04 and Visual C++ 2010.",Design-Des,yes,285cfaa19943c168f7400424a8abde37da2e143f
THRIFT,1248,description,"The ensureCanWrite function in TMemoryBuffer currently ""rebases"" the buffer pointers by subtracting the original buffer pointer from the pointer newly returned by realloc. While this seems to work fine on my linux setup(I couldn't force a reproducer), pointer subtraction between pointers that we allocated separately is an undefined operation. I have run into problems with this on platforms other than linux. I am attaching a patch that removes the undefined operation.",Design-Des,yes,6077481139933b927397c7da0088aa4678f9fb3c
THRIFT,1248,summary,pointer subtraction in TMemoryBuffer relies on undefined behavior,Design-Des,yes,6077481139933b927397c7da0088aa4678f9fb3c
THRIFT,1269,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Wed, 6 Jan 2010 02:10:19 +0000 Subject: [PATCH 01/33] thrift: handle undeclared exceptions in the async error cob Summary: This updates the generated TAsyncProcessor code to correctly handle when an undeclared exception type is passed into an error callback. It now sends back a T_EXCEPTION message to the client, just like the non-async code does. Previously the exception wouldn't be handled, causing the TEventWorker thread to exit. Test Plan: Tested changing the async sort tutorial to pass in a generic TException rather than a SortError, and verified that the client correctly received an exception. Conflicts: --- | 38 1 files changed, 32 insertions(+), 6 deletions(-)",Design-Des,yes,a94d2e5921b6bba6c0a677955edc310fb700be14
THRIFT,1269,summary,thrift: handle undeclared exceptions in the async,Design-Des,yes,a94d2e5921b6bba6c0a677955edc310fb700be14
THRIFT,1275,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Thu, 4 Mar 2010 00:53:37 +0000 Subject: [PATCH 07/33] thrift: always prefix namespaces with "" ::"" Summary: Thrift always refers to namespaces using their full name. Therefore, it should prefix them with ""::"" to avoid accidentally matching a name defined in one of the current namespaces, rather than at the top-level. For example, if ServiceB is in namespace bar, and inherits from ServiceA in namespace foo, all code emitted for ServiceB now refers to ServiceA as ::foo::ServiceA instead of just foo::ServiceA. This allows the code to compile even if a namespace ::bar::foo also exists. An extra leading whitespace is also emitted, which is needed in cases when the name is used as the first template parameter, so that the emitted code contains ""< ::"" instead of ""<::"". Test Plan: jsong reported a build problem because of this name lookup error, and this change fixed his build. I also tested building [internal fb thing] and [internal fb thing], and they both built successfully. Tags: thrift --- | 11 +++++++++-- 1 files changed, 9 insertions(+), 2 deletions(-)",Code-LQualCd,yes,0b7eeb59750fef5be018e6ec406da2da5be13c1b
THRIFT,1290,comment_0,"Hm, this patch doesn't apply cleanly for me.",Code-LQualCd,yes,37874ca8486cdce5a4b7f87c9c0b2fb3516aee5b
THRIFT,1290,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Tue, 6 Apr 2010 20:43:23 +0000 Subject: [PATCH 17/33] thrift: TNonblockingServer: clean up state in the destructor Summary: Implement the TNonblockingServer destructor, so that it closes the listen socket, destroys the event_base, and deletes any TConnections left in the connectionStack_. However, TNonblockingServer doesn't keep track of active TConnection objects, so those objects are still leaked. As part of this, I also changed the code to use event_init() rather than event_base_new(). This way we won't set the global event_base inside libevent, and we can be sure that no one else will be using it after the TNonblockingServer is destroyed. I grepped through all of [fb code base] to check for any other direct uses of event_set(), and didn't see any places that weren't also using event_base_set(). Therefore it seems like it should be safe to stop initializing the global event_base pointer. Test Plan: Tested with the test code in [a fb unittest], which creates, stops, and then deletes several Ran it under valgrind, and now it only complains about any active connections being leaked. Revert Plan: OK --- | 4 ++-- 1 files changed, 2 insertions(+), 2 deletions(-)",Code-LQualCd,yes,37874ca8486cdce5a4b7f87c9c0b2fb3516aee5b
THRIFT,1290,summary,thrift: TNonblockingServer: clean up state in the destructor,Code-LQualCd,yes,37874ca8486cdce5a4b7f87c9c0b2fb3516aee5b
THRIFT,1314,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Mon, 19 Apr 2010 19:09:16 +0000 Subject: [PATCH 25/33] thrift: add TProcessorFactory Summary: Adds a new TProcessorFactory class, and update all server classes to store a TProcessorFactory instead of a TProcessor. is called once for each newly accepted connection. This will allow a separate processor to be created for each connection, if desired. For now, everything always uses so all connections still end up using the same TProcessor. Some of the servers don't handle it well if getProcessor() throws an exception. TNonblockingServer may leak the fd and a TConnection object. TSimpleServer will exit and stop serving. However, this is no worse than the existing behavior if Test Plan: Ran the test code from [a fb unittest]. Revert Plan: OK Conflicts: --- | 39 | 12 ++++++-- | 31 | 9 +++++- | 7 ++++- | 5 +++- 6 files changed, 89 insertions(+), 14 deletions(-)",Code-LQualCd,yes,6dd9cd0e3bb0bac0d4a70594956d035b75d4d7c8
THRIFT,1349,description,There are a couple of spurious calls which can lead to lots of output when talking between slightly different versions of generated code. For instance launching a server which returns a new field to a client will result in the client printing out information about an unknown fid. This leads to lots of logging which you probably don't want. I'd like to remove these unless anyone in dev objects.,Design-Des,yes,4fb2706ecf74f533f71fa4ceab15db984fd13244
THRIFT,1349,summary,Remove unnecessary print outs,Code-LQualCd,yes,4fb2706ecf74f533f71fa4ceab15db984fd13244
THRIFT,1440,comment_0,"Thanks for Reporting that, we like to apply your patches to improve packaging and build procedure in general. The Build Job for Debian Packages is here: currently we only hav a amd64 at Apache Build Infrastructure... Do some simply tricks or cross compile approaches exist to build more architectures on that build server?",Design-Des,no,
THRIFT,1440,description,"A listing of detectable policy problems in the thrift Debian packaging (in contrib/) can be found with a lintian run: I'll note some of them here for posterity. h3. thrift source: libthrift-dev on libthrift0 The libthrift-dev package should have a versioned dependency on libthrift0, i.e., in debian/control: h3. thrift source: build-depends You don't need the ""build-essential"" bit in Build-Depends. h3. thrift-compiler: Syntax is a bit off in debian/control for the Description fields; I'll attach a patch. h3. thrift-compiler: usr/bin/thrift You need a man page for /usr/bin/thrift. h3. python-thrift-dbg: python-thrift-dbg = The python-thrift-dbg package should be in Section: debug. h3. python-thrift-dbg: dir-in-usr-local usr/local/lib/ Debian packages shouldn't be shipping anything in /usr/local; that's supposed to be reserved for the local system admin. There isn't much reason for this anyway; the dirs being shipped by python-thrift-dbg here are empty. h3. libthrift-ruby: libthrift-ruby = The libthrift-ruby package should be in Section: ruby. Also, according to , it looks like Ruby packages are undergoing a name change in the current Debian testing suite. libthrift-ruby probably needs to become ruby-thrift and switch to using gem2deb. h3. libthrift-ruby: This will probably be addressed under THRIFT-1421. h3. libthrift0: libthrift-c-glib0 This is complaining because the package name of a library package should usually reflect the soname of the included library (see [chapter 5 of the Debian Library Packaging for more info). Something is fishy here, though. Did you intend to distribute the c-glib library as ""libthrift0""? If so, where is the cpp library supposed to go? I don't think I see it after a quick search through the packages. h3. php5-thrift: See the lintian explanation for detailed info. Basically, you need some extra Sauce to add a dependency to php5-thrift on a PHP with a compatible API version. h3. libthrift-java: libthrift-java = libthrift-java should be Section: java h3. libthrift-cil: libthrift-cil = libthrift-cil should be Section: cli-mono h3. libthrift-cil: Thrift.dll shouldn't have its executable bit set. h3. libthrift-perl: usr/usr/ Yeah, installing into /usr/usr/local/lib is kinda wacko. Ought to be /usr/lib. ---- And as a final note, a lot of the packaging here could be pretty greatly simplified and better future-proofed using the Debhelper 7 command sequencer (""{{dh}}"").",Code-LQualCd,no,8516f58b77a7911c42561d7cc53024fbfab9cea9
THRIFT,1452,comment_0,"Line 1514 of t_cpp_generator.cc in the function has an unused variable ??ttype??, any reason for this or can it be removed? t_type *ttype =",Code-DedCd,yes,e1d2458f1a84c1e975d8b73260324d7ca823bf75
THRIFT,1452,comment_1,Nope no reason it looks unused in our codebase as well.,Code-DedCd,yes,e1d2458f1a84c1e975d8b73260324d7ca823bf75
THRIFT,1452,description,"From Mon Sep 17 00:00:00 2001 From: Adam Simpkins Mon, 21 Jun 2010 20:24:50 +0000 Subject: [PATCH 3/5] generate a swap() method for all generated structs Summary: Andrei mentioned it would be convenient if thrift generated swap() methods for all C++ thrift types. Apparently the ads team manually writes swap() functions for many of their thrift data types, but have to keep updating them by hand when new fields are added to the thrift interface. This updates the thrift compiler to emit swap() methods for user-defined types. For now, I decided not to emit swap() methods for the internal XXX_args, XXX_pargs, XXX_result, and XXX_presult types. Test Plan: Tested compiling serveral internal projects. I didn't actually test the generated swap() functions, but they look okay. DiffCamp Revision: 124773 Reviewed By: aalexandre Commenters: dreiss, edhall CC: davidrecordon, achao, dreiss, kholst, aalexandre, simpkins, edhall, thrift-team@lists Revert Plan: OK git-svn-id: fb thing]@30392 --- | 70 1 files changed, 67 insertions(+), 3 deletions(-)",Design-Des,yes,e1d2458f1a84c1e975d8b73260324d7ca823bf75
THRIFT,1480,description,"The python library files have some inconsistencies (different indent levels and docstring placement) and the pep8 linter produces dozens of warnings. There are also several places where tabs are used instead of spaces, which is not good. This patch addresses almost all of the pep8 issues with as little modification of the code as possible. This patch: * converts 3 instances of tabs into the correct number of spaces * removes unnecessary trailing semicolons and backslashes * changes None comparisons to be identity based, 'x != None' becomes 'x is not None' in a handful of places * removes unnecessary '== True' in one if statement * wraps lines at 80 characters and removes trailing whitespace * corrects a handful of grammar problems in docstrings (mostly to help with 80 char line wrap) * converts all the docstrings to use """""" (instead of ''' or "") and makes placement consistent * fixes pep8 warnings about missing spaces around operators, e.g. (a-b) becomes (a - b) * adjusts ordering of stdlib imports to be alphabetical (could be better still) * correct internal indent depths of methods when they switch from 2 to 4 spaces There's a mix of files that use 4-spaces for indentation, versus the majority which use 2-spaces for indentation. This patch doesn't change that. I wanted to get the code as pep8 clean as possible and touch as few lines as possible to get it there. The TType constants defined in Thrift.py have some nice vertical whitespace that isn't pep8-happy, but it looked too clean to touch so I left it unchanged. After this patch, the pep8 utility only reports two warnings: # ""indentation is not a multiple of four"" for most files (no biggie) # ""multiple spaces before operator"" in Thrift.py for the TTypes class constants The unit tests all pass with this patch.",Code-LQualCd,yes,6972041392314d526584e733781ca382a960b295
THRIFT,1480,summary,"python: remove tabs, adjust whitespace and address PEP8 warnings",Code-LQualCd,yes,6972041392314d526584e733781ca382a960b295
THRIFT,1504,summary,Cocoa Generator should use local file imports for base Thrift headers,Code-LQualCd,yes,ba021466824299c6122e26b8850759f0a17314c0
THRIFT,1533,description,should implement the {{Closable}} interface. Doing so will allow users to perform,Design-Des,yes,c9f4a35c30cfff8c98ee767dbba0f7afe62997cf
THRIFT,1533,summary,Make TTransport should be Closeable,Design-Des,yes,c9f4a35c30cfff8c98ee767dbba0f7afe62997cf
THRIFT,1583,description,The generated code has memory leaks if you create and destroy an object. I've fixed the main problem on this git repo on branch c_glib_0.8.x but commits can be moved to trunk easily. There are still some errors to control on generated code (there are comments in the code). I'm planning to fix in a short term if you validate the current patches. This git repository has also the patches sent on issues 1582 and 1578 that are also related:,Code-LQualCd,yes,c75797d9060e049692c5db1617aa9560aec939c8
THRIFT,1583,summary,c_glib leaks memory,Code-LQualCd,yes,c75797d9060e049692c5db1617aa9560aec939c8
THRIFT,1595,description,The implementation of the java test server should be the same as the languages so our cross platform tests can work.,Design-Des,yes,f42ce2a8f49cf09e695974e6cd3c434b8dda61ab
THRIFT,1799,comment_2,"Cleaned up patch, no other changes.",Code-LQualCd,yes,c880b44c5d81ade7ceb897fd55af94c0a8e5b31a
THRIFT,1799,description,"Improvements to HTML Generator * Removed HTML page name at <a href=""..."" * Option added to embed CSS inline instead of style.css. Both improvements are quite helpful in some situations, e.g. when the HTML is accessed in a way other than via the original file name, or when the separate CSS file cannot be provided along with the HTML for some reason.",Design-Des,yes,c880b44c5d81ade7ceb897fd55af94c0a8e5b31a
THRIFT,1810,comment_2,initial patch without cross language test,Test-LacTst,yes,c95d5dfb76631af655f2d905e5e514d5db6078d5
THRIFT,1810,description,"we need ruby as part of the cross language test suite, currently just a shell script (test/test.sh) running on Linux. references THRIFT-1766",Test-LacTst,yes,c95d5dfb76631af655f2d905e5e514d5db6078d5
THRIFT,1813,comment_3,"I rebased Arvind's patch, removed the duplication between autogen_comment and autogen_summary, and added a date as I previously suggested. GitHub pull request also available at:",Code-DupCd,yes,1ee7bb645d1ca5b54198d77cdc9f0517e509cc39
THRIFT,1813,description,Why? # A lot of static analysis tools understand the annotation and treat those classes differently. # Setting the date field with the date & comments field with the version of the thrift compiler might prove to be valuable for tracing the origins of the generated file. The overheads should be minimal as the retention policy of the annotation is purely at a source level,Code-LQualCd,yes,1ee7bb645d1ca5b54198d77cdc9f0517e509cc39
THRIFT,1829,description,"If you attempt to build the cpp unit tests using 'make -j x' the build will fail as there's a race between the unit test code targets depending on the thrift-generated code targets. I think the real fix would be to use one of the approaches described in the 'Multiple Outputs' section of the automake manual (see I experimented with one of them and it seemed to help but never got it quite working. However an easier workaround is to simply disable parallel builds by using the "".NOTPARALLEL"" special target which forces make to run serially. from .NOTPARALLEL If .NOTPARALLEL is mentioned as a target, then this invocation of make will be run serially, even if the ??-j?? option is given. Any recursively invoked make command will still run recipes in parallel (unless its makefile also contains this target). Any prerequisites on this target are ignored. That's the approach I ended up taking as I'm short on time.",Design-Des,yes,c095919087adc9508300ec6e9cdcd58cf147a207
THRIFT,1842,summary,Memory leak with Pipes,Code-LQualCd,yes,b64a774b2fbfab034c0b7fff1641a46d8123d19f
THRIFT,1873,description,"The binary protocol factory takes strict read/write flags as (optional) arguments, but did in fact never use them.",Code-DedCd,yes,5cb0d22a03c709ec9f581a615b9274ab765cea26
THRIFT,1924,summary,Delphi: Inconsistency in serialization of optional fields,Design-Des,yes,0f8acc5697d2ad251fccf469cad5141887626b2d
THRIFT,1932,comment_1,"Hi Roger, here is the patch you requested. It does, however, only suppress the type-punned pointer warning. Since I'm new to Thrift, I'm quite unsure if the sequence of bytes read from file handle 'fd_' is already in host byte order. If this is not the case the patch is still wrong",Design-Des,yes,d65216df190b0ff1522098c8a552594ce29feb3d
THRIFT,1932,description,"The Compilation of thrift-0.9.0 ended with the following warnings: * QA Notice: Package triggers severe warnings which indicate that it * may exhibit random runtime failures. * warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing] * warning: dereferencing type-punned pointer will break strict-aliasing rules [-Wstrict-aliasing] When looking into ... 711 = 712 713 if == 4) { 714 // 0 length event indicates padding 715 if (*((uint32_t == 0) { 716 // T_DEBUG_L(1, ""Got padding""); 717 718 continue; 719 } ... it becomes obvious that the four values casted into a pointer were previously read from the input stream by ::read() in line 661: 661 = ::read(fd_, readBuff_, readBuffSize_); Same issue here: 725 readState_.event_ = new eventInfo(); 726 While it is a two minute job fix this problem, the method looks so fragile that a clean rewrite appears to be more appropriate.",Code-LQualCd,yes,7f1df992479fdcad208889e53b8b982e2428d250
THRIFT,1982,description,"vsnprintf on Windows (visual) return -1 on failure instead of amount of bytes needed. so line like this: int need = STACK_BUF_SIZE, message, ap); won't work. Solution: ugly conditional compilation which will use microsoft specific functions... See patch for details.",Code-LQualCd,yes,d65216df190b0ff1522098c8a552594ce29feb3d
THRIFT,1999,description,"""unused arguments"" warning. suppressed by (void) x; in patch",Code-LQualCd,yes,cd54ec62492aa3f01c68910e5c942388d21c7379
THRIFT,2017,description,"In file class t_program : public t_doc { 59 public: 60 path, std::string name) : 61 path_(path), 62 name_(name), 63 out_path_(""./""), 64 { 65 scope_ = new t_scope(); 66 } 67 68 path) : 69 path_(path), 70 out_path_(""./""), 71 { 72 name_ = program_name(path); 73 scope_ = new t_scope(); 74 } In Above code at line number 65 and 73 Resource leaks happens as 1. Allocating memory by calling ""new t_scope"". 2. Assigning: ""this-3. The constructor allocates field ""scope_"" of ""t_program"" but there is no destructor in the code. destructor should deallocate memroy for t_scope. which sometimes causes Resource Leak. Possible patch: There should be destructor to release the resources allocated dynamically. ~t_program() { delete scope_; scope_ = NULL; }",Code-LQualCd,yes,09b97c78de58fea61b5dc90bd56095515bdd4f02
THRIFT,2017,summary,Resource Leak in thrift struct under,Code-LQualCd,yes,09b97c78de58fea61b5dc90bd56095515bdd4f02
THRIFT,2020,description,"A lot of windows specific files have been ""removed"" recently, but git clone still creates empty versions of those files.",Code-LQualCd,yes,d5f617f6a338fb61608f7a3f6659e05e980b3374
THRIFT,2020,summary,Thrift library has some empty files that haven't really been deleted,Code-DedCd,yes,d5f617f6a338fb61608f7a3f6659e05e980b3374
THRIFT,2021,comment_2,"+1 Applies after fixing EOL style (win=>*nix), looks good to me.",Code-LQualCd,yes,fd64c15c4fa5ab092ecdda713bae142c05aafd72
THRIFT,2021,description,"Currently, TBinaryProtocol reads into a temporary buffer, then copies the results into the resulting std::string. For large strings (like a megabyte), the copy can be a performance issue. The attached patch reads directly into the std::string instead.",Code-SlAlg,yes,fd64c15c4fa5ab092ecdda713bae142c05aafd72
THRIFT,2021,summary,Improve large binary protocol string performance,Code-SlAlg,yes,fd64c15c4fa5ab092ecdda713bae142c05aafd72
THRIFT,2032,comment_0,Patch ] does also fix a unnecessary double assignment to result2 in AcceptImpl().,Code-LQualCd,yes,102bca47f754d9c9ed6ce341c7f8f106bd2719d7
THRIFT,2032,comment_1,"Meanwhile I'm in doubt if IDisposable for the Iface is really such a good idea. To the class yes, but the Iface breaks Server code due to the missing method. The only reason for this was the using() use case, but maybe that's not enough of a reason here.",Design-Des,yes,102bca47f754d9c9ed6ce341c7f8f106bd2719d7
THRIFT,2032,comment_2,"Just to clarify, you are talking about the Client and the Processor; the IDisposable for the Client's Iface makes sense, but the IDisposable for the Processor's Iface doesn't. Unfortunately, they are the same Iface, so we need to find the correct middle ground. I'm much more in favor of having the Iface not be IDisposable, as it lowers the maintanence, with the client being explicitly IDisposable. I'm unsure of how we currently clean up state from the processor, but I don't think that IDispoable makes the most sense.",Design-Des,yes,102bca47f754d9c9ed6ce341c7f8f106bd2719d7
THRIFT,2032,comment_3,"Exactly. So lets drop the idisposable from the iface. Dont want to put too much effort in here, just need to address the leak.",Code-LQualCd,yes,102bca47f754d9c9ed6ce341c7f8f106bd2719d7
THRIFT,2032,description,"The C# client code does not correctly clean up the transport used, so the programmer has to take care on his own about this. This may even lead to a program hang in certain scenarios. Furthermore, the generated client should support IDisposable. Note that in contrast, the server side handles this automatically without any explicit manual coding. TODO: * modify generated code to add IDisposable support * modify TProtocol to add IDisposable support * update the tutorial code accordingly",Design-Des,yes,102bca47f754d9c9ed6ce341c7f8f106bd2719d7
THRIFT,2032,summary,C# client leaks sockets/handles,Design-Des,yes,102bca47f754d9c9ed6ce341c7f8f106bd2719d7
THRIFT,221,description,"The way that the current Java library and test builds are set up, they rely on an external jar that's assumed to be at some specific path. This isn't where the jar resides on my machine, so every time I want to do some building, I have to hand edit the file, and then when I want to generate a diff for posting on jira, I have to revert the changes. We should make it so the class path is configurable in a way that doesn't require checkins. There are a couple different ways to do this, but a properties file with the classpath is probably one of the easiest.",Design-Des,yes,249d7cb199b5c08e7a7a51189a733cc8fef12cf9
THRIFT,221,summary,Make java build classpath more dynamic and configurable,Design-Des,yes,249d7cb199b5c08e7a7a51189a733cc8fef12cf9
THRIFT,2210,comment_8,Unfortunately the change caused the Hive object not able to be serialized into JSON with Here is the struct: struct SkewedInfo { 1: list<string 2: list<list<string 3: map<list<string} The field is defined as a map with list as the key. The structure can be serialized and deserialized correctly with libthrift 0.9.1. This change regressed the logic. People do use TSimpleJSONProtocol to serialize Hive objects for audit logs or such. This is a blocker for thrift upgrading. Could we remove the check for the case when the key is a 'list'? Thanks!,Design-Des,no,
THRIFT,2210,description,"TSimpleJSONProtocol can emit JSON with maps whose keys are not string (which is not allowed is the JSON spec). This happens if the key in a map is anything other than a String (int, enum, etc) For example, it can emit JSON like this: which should be: I have a path that fixes this, I'll upload it shortly (still trying to get my dev environment to run the tests). Also AFAICT there is no unit test for TSimpleJSONProtocol -- I'll try and add one to the patch. Thanks! Alex",Test-LacTst,yes,38b453be5a015b7aaefcd91b4e261e53e0e211c2
THRIFT,2225,description,destroy the SSLContext before cleanup the openSSL to avoid recreating some openssl internal resource.,Design-Des,yes,301dfa94d6465244d5970e2abdc0650b386468d5
THRIFT,2227,summary,Thrift compiler generates spurious warnings with Xlint,Code-LQualCd,yes,4ccc24f6214f3041af5e564322382df1d84bf935
THRIFT,2246,comment_3,Some edge cases still produce warnings.,Code-LQualCd,yes,0ec155e1608c2909183b7c5e0b08a4a80579b4bd
THRIFT,2246,description,"if enum is not set, ToString() returns a first enum value, but it should indicate that field is empty enum Distance { DISTANCE_1 = 0, DISTANCE_2 = 1, } struct RaceDetails { 1: optional Distance distance, } c#: new result: expected: )""",Code-LQualCd,yes,088c26b40ccf747eaa5200727c9bacdc9288fb35
THRIFT,2263,description,Previously Thrift only generated high-quality hashCode implementations when configured due to the dependency on commons-lang3. After THRIFT-2260 we no long have this dependency and can unconditionally give the better implementation.,Design-Des,yes,acdac816659c88e7b8b601b4ad42dc43bf7d48e2
THRIFT,2279,description,"TSerializer copies from the transport into a 1024 length byte array and returns the array, which will effectively clips off everything after the first 1k. The attached patch instead returns a copy of the underlying memory buffer used by the transport.",Design-Des,yes,7949447efdcb2b355d3140a0d1a765e98a9a9e68
THRIFT,2328,comment_0,committed fix removing lint check for unchecked casts. compile output,Code-LQualCd,yes,567df43e80b46bf8537875c1ac817c8f9af6277b
THRIFT,2328,description,I don't like compiler warnings such as these: patches are welcome! -roger,Code-LQualCd,yes,567df43e80b46bf8537875c1ac817c8f9af6277b
THRIFT,2328,summary,Java: eliminate all compiler warnings,Code-LQualCd,yes,567df43e80b46bf8537875c1ac817c8f9af6277b
THRIFT,2328,comment_2,@  we should keep this check: "Some input files use unchecked or unsafe operations." ;-r,Code-LQualCd,yes,98d9ef2bd675e16dde9304061f71b6435caa5cf8
THRIFT,240,description,"Now that we have a deep copy constructor, TBase should implement cloneable.",Design-Des,yes,4371038cd1e1a0a0ef5283bfd540c7623fbe7514
