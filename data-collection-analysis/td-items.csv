td_item	classification	project	issue_number	issue_type	text	if_fixed	fixed_commit_hash	introduced_commit_hash
HBASE-21160|0	Design-Des	hbase	21160	comment_1	Since the catch block currently re-throws IOException, that means the catch block is no longer needed. Please run with the change locally before attaching patch. Thanks	yes	1cf920db4395926c634ed0946a57dfa52227472f	-
HBASE-21160|0	Design-Des	hbase	21160	comment_2	Hi I found so many re-throws blocks in the file of . Should we resolve it all?	yes	1cf920db4395926c634ed0946a57dfa52227472f	-
HBASE-20975|0	Code-LQualCd	hbase	20975	comment_0	+1 on removing it for now. We have optimized too much before getting things correct... Lets keep the logic simple first. Also, there are some bad style issues. At least lets remove the space between stackTail and --, it looks like -- And do not do assignm	yes	a07e755625382d3904c935c21a4f240ede6d2f43	-
HBASE-20975|1	Test-LCvg	hbase	20975	comment_4	Please include a small change in hbase-server module so we can run more tests?	no	-	-
HBASE-20975|2	Design-Des	hbase	20975	comment_8	The failed UTs are related, upload a patch to fix it. My fix reveals a bug which exists all the time... the acquire and release lock logic in procedure is a bit of mess here... For different procedures, we have holdLock() set to true of false, for procedures which holdLock() == true, we need to override hasLock() method to test whether we have the lock... when executing procedure, we acquire the lock and release it immediately. And when the procedure is success, we release the lock again in It works, since for procedure with holdLock = true, we call releaseLock() with force = false the first time, the lock wont release here, but in where we call releaseLock() with force = true, the lock will be released there. For procedure with holdLock = false, it also works since when the first time we call releaseLock() with force = false, the lock will release, and the second time we call relseaseLock in we only release it lock if procedure with holdLock=true. So releasing lock wont called twice But when rolling back, we acquire the lock but only release the lock for procedure with holdLock = true... *Rolling back for procedures with holdLock=false(e.g. most of the Table Procedures) will never release the lock...*	yes	a07e755625382d3904c935c21a4f240ede6d2f43	-
HBASE-20975|3	Code-MTCor	hbase	20975	description	Find this one when investigating HBASE-20921, too. Here is some code from executeRollback in You can see my comments in the code above, reuseLock can cause the procedure executing(rollback) without a lock. Though I havent found any bugs introduced by this issue, it is indeed a potential bug need to fix. I think we can just remove the reuseLock logic. Acquire and release lock every time. Find another case that during rolling back, the procedures lock may not be released properly: see comment:	yes	a07e755625382d3904c935c21a4f240ede6d2f43	-
THRIFT-4830|0	Code-SlAlg	thrift	4830	description	ITNOA I think it useful and have some performance benefits to generate to_string function for enum beside of operator<< overloading.	yes	6a61dfabbf6ae2fa9fbbc3996590ebdbe38e569f	-
THRIFT-1745|0	Test-LacTst	thrift	1745	comment_0	Frederic,Thank you for your work on this. Would you please attach the patch for this to the ticket. We can not accept gist or pull requests from github due to ASF licensing requirements. Once the test cases are added ill gladly review and commit this patch.	yes	85fb6de7f4c1ea6260f98bc24401593e8c974bc7,05ab89a1286049567e8d6ada1833a7d75179a365	-
CAMEL-8091|0	Design-Des	camel	8091	description	The does not consider the context property that is supposed to limit the size of the logged payload. It is possible to set a maxChars on the but that has a different semantics (limits the length of the formatted exchange,not of the message payload) and is complicated to set in some cases (e.g. in the case of the default error handler) The attached extension also honors the context property when formatting the exchange.	yes	43d02628287c0623672714a016e324d6da36d71c	-
IMPALA-8005|0	Design-Des	impala	8005	description	Currently,we use the same hash seed for partitioning exchanges at the sender. For a table with skew in distribution in the shuffling keys,multiple queries using the same shuffling keys for exchanges will end up hashing to the same destination fragments running on particular host and potentially overloading that host. We should consider using the query id or other query specific information to seed the hashing function to randomize the destinations for different queries. Thanks to  for pointing this problem out.	yes	df6196e064bc7453bee8c7e644bb591391ee3ce2	-
CAMEL-11524|0	Code-LQualCd	camel	11524	comment_0	No there is not,your workaround is to not use $ in the file name,which also is a bad habit to do so. The source code needs to be patched where you need to quote the file name in the GenericFileEndpoint method via You are welcome to work on a github PR to fix this	yes	1fa638ec156135ef9d5c6bb8684b81459c6c7c82	-
CAMEL-11524|0	Code-LQualCd	camel	11524	comment_1	Hi Claus Thanks for the response. The source of the \$\ is Ola Hallengren\s SQL maintenance script and that in turn is escaping a \\\\ in a database engine name. It seems like a better idea to fix the Camel code and make it more robust. I will look into providing a patch for this. Regards,Saycat	yes	1fa638ec156135ef9d5c6bb8684b81459c6c7c82	-
CAMEL-1173|0	Code-LQualCd	camel	1173	comment_1	Put in an improved scatter-gather example in revision 740056. The problem with the first example was that it was not dynamic enough. Also added wiki docs here	yes	f6cbe6529a1084383b72c06ab215a28dee3ebed7	-
camel-12166|0	Code-CpxCd	camel	12166	comment_3	It end up calling some rollback code, that performs the disconnect. Its a bit more complicated as ftp extends file component. But as said you use an old version of Camel we dont support anymore.	no	-	-
camel-12414|0	Code-LQualCd	camel	12414	comment_2	Okay it works fine with an unit test that does not use pax-exam. Its some weird osgi stuff, it may not start/run properly via pax-exam inside the osgi/karaf container with the unit test.	no	-	-
camel-1734|0	Test-LacTst	camel	1734	comment_0	"Hi Fernando, Thanks for the contribution. I just have quick review of your code, there is no unit test, can you add them ? Since Progress OpenEdge is a commercial product , I don't know if we could download the jar from public mvn repository and use it freely, can you clarify it ? If not , we need to find a way to accept your code. BTW, the codes have no Apache License declaration, you can take the other camel component module as an example. Willem"	no	-	-
camel-1734|0	Test-LacTst	camel	1734	comment_1	Regarding the dependency on Progress OpenEdge, it is only available to their customers, which will need instructions for wrapping and deploying it in SMX. I will work on the unit tests and Apache License issues right away.	no	-	-
camel-1734|0	Test-LacTst	camel	1734	comment_3	I don't have any problem with that, let me know how to proceed. Updated the submission with the Apache License (are we going to stick to it?), no unit tests yet.	no	-	-
camel-2559|0	Test-LCvg 	camel	2559	comment_3	"I think I got it covered now, will do more testing. But I have reworked camel-http and camel-jetty a little big to - use endpoint configured options over component configured - not mess with the component configured - using a single shared as that is whats meant - properly registering http/https with correct port number on SchemeRegistry on,test_debt,low_coverage"	yes	9041239036fd53a40ff6708e982882a77182e5b	-
camel-3112|0	Design-Des	camel	3112	description	To make it easier to read/write files using that given encoding. Currently you have to set that as a property on the Exchange using setProperty in the DSL. But when using @Produce or @Consume you only got the endpoint uri. So having this in the component makes it easier.	yes	e11a4a3749e23953e00ccfcdfdd05c36ac610130	-
camel-3125|0	Design-Des	camel	3125	summary	The problem is with the default implementation of When an error occurs rollback() is called, and this method just logs the error and returns false, which means the polling will not be retried for that execution of the run() method. This means it will be retried after the delay. I created an implementation that suspends the Consumer, which seems like acceptable default behavior. It also allows for extension and adding some hooks for custom stuff. In order for that to be useful it should be made easier to set your own implementation.	yes	1fad136fe822819607c0de79df61b3c9ace4f8a5	-
hadoop-12002|0	Code-LQualCd 	hadoop	12002	comment_3	It'd be better to loop over all the executables and print out all of them that aren't found, then give the -1 and return. Right now, if someone is missing several of them they'll probably have to run through multiple times.	no	-	-
hadoop-12002|1	Code-CpxCd 	hadoop	12002	comment_3	"* Generate a -1 jira table for every missing executable. This simplifies the code and gets rid of a lot of excess variables. * We can drop the extra line feed, so just just use echo instead of the more complex printf * This isn't java, we don't need camelCase or really long names.  ) (e.g., findbugsExecutable) [Yes, i recognize I didn't purge them out the first time, but we should make the effort to get rid of them when we can.] * Remove the extra check: -x should fail if the file doesn't exist."	no	-	-
hadoop-12496|0	Design-Des	hadoop	12496	description	"hadoop-aws jar still depends on the very old 1.7.4 version of aws-java-sdk. In newer versions of SDK, there is incompatible API changes that leads to the following error when trying to use the S3A class and newer versions of sdk presents. This is because S3A is calling the method with ""int"" as the parameter type while the new SDK is expecting ""long"". This makes it impossible to use kinesis + s3a in the same process. It would be very helpful to upgrade hadoop-awas's aws-sdk version. $iwC$$iwC.<init at $iwC.<init at <init at .<init at .<clinit at .<init at .<clinit at $print(<console at Method)"	no	-	-
hadoop-13158|0	Code-LQualCd 	hadoop	13158	description	,The {{cannedACL}} field of {{S3AFileSystem}} can be {{null}}. The {{toString}} implementation has an unguarded call to so there is a risk of	yes	08ea07f1b8edbc38c99015c81a62ca127a247bf7	-
hadoop-14351|0	Code-LQualCd 	hadoop	14351	comment_4	Thanks  for reviewing the patch. Attached another patch containing the checkstyle fixes excluding the fix for the {{FileLength}} related warning. The below findbugs warning is unrelated to changes done in the patch which was introduced in [Commit of HADOOP-10809	yes	8b5f2c372e70999f3ee0a0bd685a494e06bc3652	-
hadoop-14942|0	Code-LQualCd 	hadoop	14942	comment_1	LGTM +1 cleanup code is always good to make robust	yes	f36cbc847560d53e7955ced9ce7ce2773c805793	-
hadoop-14942|0	Code-LQualCd 	hadoop	14942	description	Over in HBASE-18975, we observed the following: came from second line below: in which case jobFS was null. A check against null should be added.	yes	f36cbc847560d53e7955ced9ce7ce2773c805793	-
hadoop-14942|0	Code-LQualCd 	hadoop	14942	summary	DistCp#cleanup() should check whether jobFS is null	yes	f36cbc847560d53e7955ced9ce7ce2773c805793	-
hadoop-6151|0	Code-LQualCd 	hadoop	6151	comment_5	"* The unit test should use JUnit4 test annotations instead of JUnit3 TestCase * looks useful for debugging, but should probably be left out * The static \*Bytes fields should be final * The @return docs for ""needsQuoting"" could be more explicit"	yes	366b1b1dd6f1ade1996c7c0eec1aca185c68d6cb	-
hadoop-6839|0	Code-LQualCd 	hadoop	6839	comment_4	Looks good overall. Please fix the formatting (white space are inconsistent) and make exception and error messages more meaningful. Submit the patch for verification whenever is ready.	yes	a73dfef140f293cd191f56e412f0a7328faea437	-
hadoop-6839|0	Code-LQualCd 	hadoop	6839	description	Develop a new method for getting the user list. Method signature is public ArrayList<StringAdd new attribute in system-test.xml file for getting userlist path. For submitting the jobs as different user a proxy user id is needed. So,get the available users from a userlist and then pass the user as proxy instead of hardcoding user id in a test.	yes	a73dfef140f293cd191f56e412f0a7328faea437	-
hadoop-6925|0	Test-LCvg 	hadoop	6925	comment_0	Patch fixes the read() implementation to correctly mask with 0xff before upcasting to int. Also augments the unit test to check the single-byte read() function - the new test fails before this patch.	yes	19e46e358e31ef730d7486c517a61aff5f3d0149	-
hadoop-6988|0	Code-LQualCd 	hadoop	6988	comment_1	At a minimum, -1 on the environment variable. Shouldn't HADOOP_CLIENT_OPTS be sufficient for passing extra -D params? We have an abundance of environment variables that users can't handle as it is.	no	-	-
hadoop-6988|0	Code-LQualCd 	hadoop	6988	comment_3	"Grr. I really wish we'd stop creating pet environment variables. This is ridiculous. Can we remove this env var as part of this JIRA? What takes precendence the env var or the jobconf setting? What is the interaction? If the answer is ""we have to look at the code"" then we've failed. It makes much more sense to have to support a comma delimited set (to be consistent with the rest of the job conf. Never mind that colon is the traditional directory delimiter on OS X.)"	no	-	-
hadoop-6988|0	Code-LQualCd 	hadoop	6988	comment_4	The environment variable should *not* be multi-valued. It is used to communicate the job's token store to sub-processes of the task. Since a task can't be in more than one job, there isn't any need. What is the use case for having multiple token files? The rest of the lists use commas, so this should be the same. Wouldn't it be easier to write a tool that allows you to combine multiple token files together into a single one?	no	-	-
hadoop-6988|1	Design-Des	hadoop	6988	comment_9	I'm sorry, but it is completely short sighted to have a single use env var like this. If we need to modify the tasks environment for something else, are we going to introduce another environment variable? How many are too many?	no	-	-
hbase-12673|0	Test-LCvg 	hbase	12673	comment_4	Hi , I don't think this unit test covers the case I'm concerned about. This unit test covers a fairly coarse grained happening -- a read is happening and inbetween read operations a table delete happens. What I'm concerned about is finer grained -- Let's say I'm reading a from a snapshot mob file which is pointing to the mob in the original dir. While this happens (while still in the middle of the read operation) a table deletion on the original table happens which move the original mob file to the archive. The read operation may fail (can't find more data from the file). With the HFilelink, we'd intercept that exception and then point the read to the moved location if the file ends up in the correct place. With the current code, I believe we get an IO exception and fail to return the proper data, or return an error. I'm in the process of crafting a rig that will constantly exercise these concurrently and hopefully will be able to produce a stack trace when this fails in a day or two.	no	-	-
hbase-12673|1	Design-Des	hbase	12673	comment_6	Thanks for the pointer in HMobStore @ 312. The retry there should address the situation I was concerned about. Looking at that code again one other concern comes up -- do you know if line 319 in there will throw another exception if we got the FNFE on line 313? (we'd have an open file instance that got moved  not sure what would happen on close on line 319). Can we change it so that we capture the other exceptions that could be caught there [1] and then try the next location? HFileLink essentially makes this file redirection mechanism transparent and would potentially make the code easier to follow. I'd prefer it if we could use that code so if we find other cases we can just fix it in one centralized place. [1]	no	-	-
hbase-12673|1	Design-Des	hbase	12673	comment_9	Ok, my main ask is to use the wrapping that the FileLink/HFileLink provides when a reader to the file is opened. You don't need to use the funny encoded name sine we aren't in a situation where we have a placeholder file present. We are in a place where we open a file and it could move to the archive. I spent a little bit of time in there and between the StoreFile, StoreFileInfo, Reference and HFileLink it is a bit messy. I'm planning to spend a little bit of time to refactor/clean things up in there so we have only have one mechanism to use. I think the short term solution is to add the other exception checks -- e.g. handle more than just the FNFE and catch these two other exception cases.	no	-	-
hbase-13871|0	Architecture-VioMod	hbase	13871	comment_3	"Do we have to have a class named FirstOnRowFakeCell at top level of our class hierarchy? Is it only used in CellUtil? Could it be an inner class of it? Any reason for this change? 5252 this.isScan = scan.isGetScan() ? -1 : 0  5252 this.isScan = scan.isGetScan() ? 1 : 0  Otherwise, seems good."	yes	bf3924ed054a7ee05f1214af9cd695d3c036ec3c	-
hbase-13871|0	Architecture-VioMod	hbase	13871	comment_4	Yes this class need not be top level.. Let me see how we can make it inner. Yes this change is intended. Pls see the compare in isStopROw is changed. So we need this change.	yes	bf3924ed054a7ee05f1214af9cd695d3c036ec3c	-
hbase-13871|0	Architecture-VioMod	hbase	13871	comment_6	I will change it in next patch. BTW , we might add some more similar classes in future. This is byte[] backed fake Cell impl and we will have a BB backed one also. Also LastOnRowFakeCell will come in.. JFYI. I will keep this class(es) private audience only even if CellUtil is public exposed. Thanks Stack.	yes	bf3924ed054a7ee05f1214af9cd695d3c036ec3c	-
Hbase-14161|0	Test-LacTst	hbase	14161	comment_4	Ok. The tests in hbase-spark are run as regular unit tests now after HBASE-17574. We may need to write new integration tests for the spark module. But until that is available, there is real IT yet.	no	-	-
Hbase-14161|0	Code-DupCd	hbase	15490	description	Currently there're two in our branch-1 code base (one in package, the other in and both are in use. This is a regression of HBASE-14969 and only exists in branch-1. We should remove the one in and change the default compaction throughput controller back to to keep compatible with previous branch-1 version Thanks  for pointing out the issue.	no	-	-
Hbase-14161|0	Code-DupCd	hbase	15490	summary	Remove duplicated in branch-1	no	-	-
Hbase-16157|0	Test-FlakyTst	hbase	16157	comment_6	lgtm. Is the test stable enough? Looks like it might end up being a flaky test.	yes	5a7c9939cbe115c0d12d9cfe8bf9f3b3d11ac69e	-
Hbase-19815|0	Test-FlakyTst	hbase	19815	summary	Flakey	yes	581fabe7b2177a090af33517f2f7cb1cdab2c64b	-
Hbase-20100|0	Design-Des	hbase	20100	comment_4	Linked to a follow-on issue. The AssignProcedure is too coarsely grained (feedback from  that makes sense). HBASE-20103 is about breaking it up to do finer steps. Will do in follow-on.	no	-	-
Hbase-20100|1	Test-LCvg 	hbase	20100	description	Failed in the nightly in interesting way. A subprocedure of enable table is assigning regions. The test is doing a kill of the procedure store to ensure we can handle all state transtions even in face of failure. In this case, the kill of PE framework came in the finish of the assign procedure AFTER we'd updated hbase:meta and the internal AM state moving Region from OPENING to OPEN. Rerunning this step on restoration of the AMv2 WAL Store results in....	yes	ba063abd2f4b0aaa0622a8665c4ba96ed31eafb8	-
Hbase-20100|2	Test-FlakyTst	hbase	20100	summary	flakey	no	-	-
Hbase-20108|0	Design-Des	hbase	20108	comment_5	looks like jline shows up in the modules now. the rest of the approach looks really good, though! I assume this works both in dev tree and out of a bin tarball?	no	-	-
Hbase-20696|0	Code-LQualCd 	hbase	20696	summary	Shell list_peers print useless string	no	-	-
Hbase-21589|0	Test-FlakyTst	hbase	21589	comment_2	It is very strange, it never failed in my environment. , can you upload an output or something, I can't find the failing test in jenkins or Flaky test board.	yes	68b5df00951d3ee55efaa6068f4530dca17eae1f	-
IMPALA-1082|0	Code-LQualCd 	impala	1082	comment_2	We are running CDH 4.4 and would need to upgrade to a later build. This appears to be occurring on only 1 or 2 of our nodes on the cluster and thus I believe is data centric. Are they any workarounds or ways to identify/cleanup the data?	no	-	-
IMPALA-2724|0	Test-FlakyTst	impala	2724	comment_0	Dan, you seem to be the last person who touched this test. It appears the flakiness of the test had to do with the general non-determinism of impala memory usage. I believe that's the same case for IMPALA-2728.	yes	968c61c940fe78faa08e8cc4bd257dd2ec19dc11	-
IMPALA-326|0	Code-LQualCd 	impala	326	description	The validation for supported encodings for parquet is too strict. We also need to allow RLE and BIT_PACKED in the enums check. Also, the enums should print the strings in the error message, not the numerical values.	no	-	-
IMPALA-4485|0	Design-Des	impala	4485	description	Currently table metadata changes are locked at the table level for DML. For tables with partitions, it would help performance if concurrent inserts if locking was based at the partition level based on changes to the partition(s) affected, instead of locking the entire table.	no	-	-
IMPALA-4833|0	Design-Des	impala	4833	comment_0	There's some relevant code in the Scheduler and Coordinator. The Scheduler does a pass over all fragment instances to determine the set of hosts here: The coordinator also does something similar: It would maybe be helpful if the scheduler produced backend_params_map_ itself to make it easier to compute the per-host aggregates.	yes	6c1254656186b62e90674b4fe093a6864ccbbde5	-
IMPALA-4833|0	Design-Des	impala	4833	description	Following on from IMPALA-3748, we should consider combining the resource estimates with the schedule to accurate compute the minimum buffer requirement per daemon. Different daemons may have different resource requirements because not every fragment runs on every daemon (e.g. unpartitioned fragments, fragments with fewer scan ranges than daemons).	yes	6c1254656186b62e90674b4fe093a6864ccbbde5	-
IMPALA-5273|0	Code-SlAlg	impala	5273	description	Replacing StringCompare (which uses SSE4.2 instructions) with a call to glibc's memcmp results in a memcmp on my machine mainly uses sse4.1's ptest, after detecting at run-time that I have sse4.1 instructions available. The StringCompare benchmark is 5 years old and likely out-of-date by now. To replicate:	yes	5cab97fd7faced41bbcc2370a400cbe92c3e0128	-
IMPALA-5273|0	Code-SlAlg	impala	5273	summary	StringCompare is very slow	yes	5cab97fd7faced41bbcc2370a400cbe92c3e0128	-
IMPALA-530|0	Code-SlAlg	impala	530	description	I was experimenting with queries on year / month / day fields stored as strings in a TEXT data file. In addition to the need to CAST them to INT to get correct sorting during ORDER BY, I noticed that the sorting of the CAST-to-INT values was 2x faster than the original string values (this is with about 500M rows): select distinct year, month, day from raw_data_ymd order by year, month, day limit 7500; ... Returned 5124 row(s) in 102.45s select distinct year, month, day from raw_data_ymd order by cast (year as int), cast (month as int), cast (day as int) limit 7500; ... Returned 5124 row(s) in 41.87s But all the strings are very short, 1-2 bytes for the day and month fields, and 4 bytes for the year field. I would expect these could be sorted in about the same time as the corresponding integers, especially since the integer sorting in the above query has the overhead of 3 CASTs per row. I wonder if there is some latent UTF-8 processing slowing down the string sorting even though all the values are ASCII, or if the unknown-in-advance length of the strings makes the string sort harder to optimize. Could there be some special casing when the string data is known to be ASCII (i.e. like currently), and Impala could infer a binary collation? E.g. the strings could be compared 2, 4, or 8 bytes at a time as if they were integers. I first thought of this as an optimization for short string columns where the data is of consistent length, but if the strings are null-terminated and held in a buffer with a size rounded to an even number, I think the technique would apply for strings of different or unknown-in-advance lengths.	no	-	-
IMPALA-530|0	Code-SlAlg	impala	530	summary	Could ASCII string sorting be faster?	no	-	-
IMPALA-5341|0	Test-FlakyTst	impala	5341	description	"In our planner tests we want to ignore minor differences in reported file sizes to avoid flaky tests, see IMPALA-2565. However, the introduced filter also matches ""row-size="" which we do not want to filter. Relevant code is in TestUtils.java:"	yes	f9155f0d8185b3344fe16368806b463fe27bc5c7	-
IMPALA-5636|0	Test-LacTst	impala	5636	comment_0	I think just replacing 2 occurrences of with Encoding::RLE makes sense. I've manually tested with parquet-tools and it seems to work. Since parquet-tools is not in our distribution, I'm not sure how to write a test. Any suggestion?	no	-	-
IMPALA-5688|0	Test-ExpTst	impala	5688	description	Two tests ({{LongReverse}} and the base64 tests in run their tests over all lengths from 0..{{some length}}. Both take several minutes to complete. This adds a lot of runtime for not much more confidence. If instead we pick a set of 'interesting' (including powers-of-two, prime numbers, edge-cases) lengths, we can get a similar amount of confidence while significantly reducing the runtime of expr-test.	yes	1653419bd8b3748bbc0e3d5e7ffa1d412bc4b50f	-
IMPALA-5688|0	Test-ExpTst	impala	5688	summary	Speed up a couple of heavy-hitting expr-tests	yes	1653419bd8b3748bbc0e3d5e7ffa1d412bc4b50f	-
THRIFT-1990|0	Architecture-ObsTech	thrift	1990	comment_1	Ah ha, turns out Ubuntu's thrift package is v 0.8, which is quite outdated and doesn't include exception support.	no	-	-
THRIFT-2555|0	Design-Des	thrift	2555	description	produces which starts to become annoying the larger the gaps between the numbers are.	no	-	-
THRIFT-2561|0	Code-LQualCd 	thrift	2561	description	"When an enumerated type is translated to objective-c, the type is replaced with simply ""int"". This takes away half the use of an enumerated type as a type. For instance, it is both clearer and a more precise translation to see ""PersonID"" as a field type in a method than ""int"". However, the objective-c compiler does not typedef the enum and as a result essentially forgets the type that is declared in the thrift IDL."	no	-	-
THRIFT-2568|0	Design-Des	thrift	2568	description	Enhance the C# TLS-Transport with a paramater to give it a specific certificate handler function. This enables to easely accept untrusted certificates or accept a specific certificate, which can be useful while debuging.	yes	7b11fec0c53b3231a472e008dfbb285d1aac44df	-
THRIFT-2666|0	Design-Des	thrift	2666	comment_0	Replaced {{PYTHONHASHSEED}} by a literal. Any better solution is welcome.	yes	d0bd17e7263cb8f92c21d3e1dad2ee5b5e9f79e5	-
THRIFT-2666|0	Design-Des	thrift	2666	description	The hash function introduced with THRIFT-2621 breaks with Python older than 3.2, because PYTHONHASHSEED has been introduced with 3.2. The proposed solution is to define a constant PYTHONHASHSEED as a workaround, but only if PYTHONHASHSEED is not available.	yes	d0bd17e7263cb8f92c21d3e1dad2ee5b5e9f79e5	-
THRIFT-2758|0	Code-LQualCd 	thrift	2758	description	"Of course I had to make some content changes. The C# patch file is 354 KB, a lot of files are considered completely replaced. The Delphi patch file has ""only"" 282 KB. The ""content"" changes are (1) one indentation alignment in a batch file, and (2) one missing ASF header in a .cs file which I added. Anything else is just TABS, SPACEs and CRs. I'm going to commit that. For the records, here are the patch files."	yes	d5436f5cf7a100d89abb3d125d8f241ca7dc925e	-
THRIFT-2758|0	Code-LQualCd 	thrift	2758	summary	Whitespace fixups	yes	d5436f5cf7a100d89abb3d125d8f241ca7dc925e	-
THRIFT-568|0	Architecture-VioMod	thrift	568	description	When building thrift python libs on ubuntu, it places the files into site-packages instead of dist-utils	no	-	-
THRIFT-617|0	Code-LQualCd 	thrift	617	description	One of my thrift services does special handling upon receiving SIGHUP. This causes spurious exceptions about EINTR to be logged. Attached patch adds EINTR retry logic akin to the C++ implementation to handle this.	no	-	-
